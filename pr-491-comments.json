[{"url":"https://api.github.com/repos/tnorlund/Portfolio/pulls/comments/2594458645","pull_request_review_id":3547057551,"id":2594458645,"node_id":"PRRC_kwDOM8BQvs6apEwV","diff_hunk":"@@ -1,3 +1,21 @@\n+\"\"\"\n+Compatibility shim for legacy imports.\n+\"\"\"\n+\n+from infra.components.codebuild_docker_image import *  # noqa: F401,F403\n+\n+\"\"\"\n+Compatibility shim for legacy imports.\n+\"\"\"\n+\n+from infra.components.codebuild_docker_image import *  # noqa: F401,F403\n+\n+\"\"\"\n+Compatibility shim for legacy imports.\n+\"\"\"\n+\n+from infra.components.codebuild_docker_image import *  # noqa: F401,F403\n+","path":"infra/codebuild_docker_image.py","commit_id":"a305f6b5943e8ce8e40ab9a9d7ca3315da013746","original_commit_id":"a305f6b5943e8ce8e40ab9a9d7ca3315da013746","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Critical: Duplicate shim content and incorrect file structure.**\n\nThe compatibility shim is duplicated three times (lines 1-5, 7-11, 13-17). The file should contain only a single shim that re-exports from the new location.\n\n\nThe file should be simplified to:\n```python\n\"\"\"\nCompatibility shim for legacy imports.\n\"\"\"\n\nfrom infra.components.codebuild_docker_image import *  # noqa: F401,F403\n```\n\nRemove all duplicate content and the full class implementation that follows, as it duplicates `infra/components/codebuild_docker_image.py`.\n\n<details>\n<summary>üß∞ Tools</summary>\n\n<details>\n<summary>ü™õ Ruff (0.14.7)</summary>\n\n5-5: Unused `noqa` directive (non-enabled: `F401`)\n\nRemove unused `noqa` directive\n\n(RUF100)\n\n---\n\n11-11: Unused `noqa` directive (non-enabled: `F401`)\n\nRemove unused `noqa` directive\n\n(RUF100)\n\n---\n\n17-17: Unused `noqa` directive (non-enabled: `F401`)\n\nRemove unused `noqa` directive\n\n(RUF100)\n\n</details>\n\n</details>\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn infra/codebuild_docker_image.py around lines 1 to 18 the compatibility shim\nis duplicated three times and the file also contains a full class implementation\nthat duplicates infra/components/codebuild_docker_image.py; remove the duplicate\nblocks and any duplicate class implementation so the file contains only a single\ncompatibility shim: the module docstring and one re-export line \"from\ninfra.components.codebuild_docker_image import *  # noqa: F401,F403\".\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:ocelot -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commit 6ae1f43","created_at":"2025-12-06T03:01:34Z","updated_at":"2025-12-06T03:20:44Z","html_url":"https://github.com/tnorlund/Portfolio/pull/491#discussion_r2594458645","pull_request_url":"https://api.github.com/repos/tnorlund/Portfolio/pulls/491","_links":{"self":{"href":"https://api.github.com/repos/tnorlund/Portfolio/pulls/comments/2594458645"},"html":{"href":"https://github.com/tnorlund/Portfolio/pull/491#discussion_r2594458645"},"pull_request":{"href":"https://api.github.com/repos/tnorlund/Portfolio/pulls/491"}},"reactions":{"url":"https://api.github.com/repos/tnorlund/Portfolio/pulls/comments/2594458645/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":1,"start_side":"RIGHT","line":null,"original_line":18,"side":"RIGHT","author_association":"NONE","original_position":18,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/tnorlund/Portfolio/pulls/comments/2594458646","pull_request_review_id":3547057551,"id":2594458646,"node_id":"PRRC_kwDOM8BQvs6apEwW","diff_hunk":"@@ -12,17 +30,51 @@\n - Supports multi-stage builds with layer caching\n \"\"\"\n \n-import hashlib\n+# pylint: disable=import-error\n+\n import json\n-import os\n import shlex\n from pathlib import Path\n from typing import Any, Dict, Optional\n \n import pulumi\n-import pulumi_aws as aws\n import pulumi_command as command\n+from build_utils import (\n+    compute_hash,\n+    make_artifact_bucket,\n+    make_log_group,\n+    resolve_build_config,\n+)\n+from buildspecs import docker_image_buildspec","path":"infra/codebuild_docker_image.py","commit_id":"a305f6b5943e8ce8e40ab9a9d7ca3315da013746","original_commit_id":"a305f6b5943e8ce8e40ab9a9d7ca3315da013746","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Broken imports will cause `ImportError` at runtime.**\n\nThese imports use bare module names without the `infra.shared.` prefix:\n\n```python\nfrom build_utils import (...)\nfrom buildspecs import docker_image_buildspec\n```\n\nThis will fail unless the module is executed from within `infra/shared/`. Since this is a compatibility shim, the entire implementation below line 18 should be removed‚Äîit duplicates the canonical implementation in `infra/components/codebuild_docker_image.py`.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn infra/codebuild_docker_image.py around lines 42-48 the file contains broken\nbare imports (from build_utils / buildspecs) and duplicates the canonical\nimplementation; remove the duplicated implementation below line 18 and replace\nthis shim file with a simple re-export of the canonical implementation at\ninfra/components/codebuild_docker_image.py (i.e. delete the current\nimplementation and ensure the file only imports and re-exports the real module\nso we no longer rely on local bare imports).\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:ocelot -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commit 6ae1f43","created_at":"2025-12-06T03:01:34Z","updated_at":"2025-12-06T03:20:52Z","html_url":"https://github.com/tnorlund/Portfolio/pull/491#discussion_r2594458646","pull_request_url":"https://api.github.com/repos/tnorlund/Portfolio/pulls/491","_links":{"self":{"href":"https://api.github.com/repos/tnorlund/Portfolio/pulls/comments/2594458646"},"html":{"href":"https://github.com/tnorlund/Portfolio/pull/491#discussion_r2594458646"},"pull_request":{"href":"https://api.github.com/repos/tnorlund/Portfolio/pulls/491"}},"reactions":{"url":"https://api.github.com/repos/tnorlund/Portfolio/pulls/comments/2594458646/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":42,"start_side":"RIGHT","line":null,"original_line":48,"side":"RIGHT","author_association":"NONE","original_position":44,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/tnorlund/Portfolio/pulls/comments/2594458647","pull_request_review_id":3547057551,"id":2594458647,"node_id":"PRRC_kwDOM8BQvs6apEwX","diff_hunk":"@@ -0,0 +1,1011 @@\n+#!/usr/bin/env python3\n+\"\"\"\n+codebuild_docker_image.py\n+\n+AWS CodeBuild-based Docker image builder for Lambda functions.\n+Offloads Docker builds to AWS for faster pulumi up times.\n+\n+Similar to EcsLambda but for Docker images:\n+- Fast `pulumi up` (async by default)\n+- Simple architecture: S3 ‚Üí CodePipeline ‚Üí CodeBuild ‚Üí ECR ‚Üí Lambda\n+- Maintains Pulumi state\n+- Supports multi-stage builds with layer caching\n+\"\"\"\n+\n+# pylint: disable=import-error\n+\n+import json\n+import shlex\n+from pathlib import Path\n+from typing import Any, Dict, Optional\n+\n+import pulumi\n+import pulumi_command as command\n+from pulumi import ComponentResource, Output, ResourceOptions\n+from pulumi_aws import config, get_caller_identity\n+from pulumi_aws.codebuild import (\n+    Project,\n+    ProjectArtifactsArgs,\n+    ProjectCacheArgs,\n+    ProjectEnvironmentArgs,\n+    ProjectEnvironmentEnvironmentVariableArgs,\n+    ProjectLogsConfigArgs,\n+    ProjectLogsConfigCloudwatchLogsArgs,\n+    ProjectSourceArgs,\n+)\n+from pulumi_aws.codepipeline import (\n+    Pipeline,\n+    PipelineArtifactStoreArgs,\n+    PipelineStageActionArgs,\n+    PipelineStageArgs,\n+)\n+from pulumi_aws.ecr import Repository, RepositoryImageScanningConfigurationArgs\n+from pulumi_aws.iam import Role as ROLE\n+from pulumi_aws.iam import RolePolicy\n+from pulumi_aws.lambda_ import (\n+    Function,\n+    FunctionEnvironmentArgs,\n+    FunctionEphemeralStorageArgs,\n+    FunctionFileSystemConfigArgs,\n+    FunctionImageConfigArgs,\n+    FunctionVpcConfigArgs,\n+)\n+\n+from infra.shared.build_utils import (\n+    compute_hash,\n+    make_artifact_bucket,\n+    make_log_group,\n+    resolve_build_config,\n+)\n+from infra.shared.buildspecs import docker_image_buildspec\n+from infra.utils import _find_project_root\n+\n+PROJECT_DIR = _find_project_root()\n+\n+\n+class CodeBuildDockerImage(ComponentResource):\n+    \"\"\"AWS CodeBuild-based Docker image builder with ECR push and Lambda update.\n+\n+    This component:\n+    - Creates ECR repository\n+    - Uploads Dockerfile + build context to S3\n+    - Runs CodeBuild to build multi-stage Docker image\n+    - Pushes to ECR with content-based tags\n+    - Updates Lambda function with new image URI\n+    - Fast pulumi up (async by default, sync in CI)\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        name: str,\n+        *,\n+        dockerfile_path: str,  # Path to Dockerfile relative to project root\n+        build_context_path: str,  # Path to build context (usually project root)\n+        source_paths: Optional[\n+            list[str]\n+        ] = None,  # Specific paths to include in build\n+        lambda_function_name: Optional[\n+            str\n+        ] = None,  # If provided, updates Lambda\n+        lambda_config: Optional[Dict[str, Any]] = None,  # Lambda configuration\n+        build_args: Optional[Dict[str, str]] = None,\n+        platform: str = \"linux/arm64\",\n+        sync_mode: Optional[bool] = None,\n+        lambda_aliases: Optional[\n+            list[str]\n+        ] = None,  # Pulumi aliases for Lambda rename\n+        opts: Optional[ResourceOptions] = None,\n+    ) -> None:\n+        super().__init__(f\"codebuild-docker:{name}\", name, {}, opts)\n+\n+        self.name = name\n+        # Keep paths relative - they'll be resolved from workspace root\n+        self.dockerfile_path = dockerfile_path\n+        self.build_context_path = build_context_path\n+        self.source_paths = (\n+            source_paths or []\n+        )  # Specific source paths for selective copying\n+        self.lambda_function_name = (\n+            lambda_function_name or f\"{name}-{pulumi.get_stack()}\"\n+        )\n+        self.lambda_config = lambda_config or {}\n+        self.build_args = build_args or {}\n+        self.platform = platform\n+        self.lambda_aliases = (\n+            lambda_aliases or []\n+        )  # Pulumi aliases for Lambda rename\n+\n+        # Configure build mode and flags\n+        (\n+            self.sync_mode,\n+            self.force_rebuild,\n+            self.debug_mode,\n+        ) = resolve_build_config(\n+            \"docker-build\",\n+            sync_override=sync_mode,\n+            ci_default_sync=True,\n+        )\n+\n+        # Calculate content hash for change detection\n+        content_hash = self._calculate_content_hash()\n+\n+        if self.sync_mode:\n+            pulumi.log.info(\n+                f\"üîÑ Building image '{self.name}' in SYNC mode (will wait)\"\n+            )\n+        else:\n+            pulumi.log.info(\n+                f\"‚ö° Image '{self.name}' in ASYNC mode (fast pulumi up)\"\n+            )\n+            pulumi.log.info(\n+                f\"   üì¶ Hash: {content_hash[:12]}... - will build only if changed\"\n+            )\n+\n+        # Create ECR repository\n+        self.ecr_repo = Repository(\n+            f\"{self.name}-repo\",\n+            image_scanning_configuration=RepositoryImageScanningConfigurationArgs(\n+                scan_on_push=True\n+            ),\n+            force_delete=True,\n+            opts=ResourceOptions(parent=self),\n+        )\n+\n+        # Setup build pipeline\n+        (\n+            _build_bucket,\n+            _upload_cmd,\n+            pipeline,\n+            _codebuild_project,\n+            pipeline_trigger_cmd,\n+        ) = self._setup_pipeline(content_hash)\n+\n+        # Push bootstrap image and create Lambda function if config provided\n+        if self.lambda_config:\n+            bootstrap_cmd = self._push_bootstrap_image()\n+            self._create_lambda_function(\n+                bootstrap_cmd, pipeline, pipeline_trigger_cmd\n+            )\n+        else:\n+            self.lambda_function = None\n+\n+        # Export outputs\n+        self.repository_url = self.ecr_repo.repository_url\n+        self.image_uri = self.ecr_repo.repository_url.apply(\n+            lambda url: f\"{url}:latest\"\n+        )\n+        # Digest is managed by CodeBuild, provide a placeholder\n+        # The actual digest is used during Lambda updates in post_build phase\n+        self.digest = pulumi.Output.from_input(\"sha256:placeholder\")\n+\n+        # Self-reference for compatibility with components that wrap this\n+        self.docker_image = self\n+\n+        self.register_outputs(\n+            {\n+                \"repository_url\": self.repository_url,\n+                \"image_uri\": self.image_uri,\n+                \"digest\": self.digest,\n+            }\n+        )\n+\n+    def _calculate_content_hash(self) -> str:\n+        \"\"\"Calculate hash of Dockerfile and relevant context files.\"\"\"\n+        paths: list[Path] = []\n+\n+        # Include Dockerfile\n+        dockerfile = Path(PROJECT_DIR) / self.dockerfile_path\n+        if dockerfile.exists():\n+            paths.append(dockerfile)\n+\n+        # If source_paths specified, hash only those paths\n+        if self.source_paths:\n+            for source_path in sorted(self.source_paths):\n+                full_path = Path(PROJECT_DIR) / source_path\n+                if full_path.exists():\n+                    paths.append(full_path)\n+\n+            # ALWAYS hash the handler directory (Lambda-specific code)\n+            handler_dir = Path(PROJECT_DIR) / Path(self.dockerfile_path).parent\n+            if handler_dir.exists():\n+                for file_path in sorted(handler_dir.rglob(\"*.py\")):\n+                    if file_path.is_file() and not any(\n+                        part.startswith(\".\") or part == \"__pycache__\"\n+                        for part in file_path.parts\n+                    ):\n+                        paths.append(file_path)\n+        else:\n+            # Hash only the files that will be included in the build context\n+            if self.build_context_path == \".\":\n+                # Lambda images - hash Python packages and handler directory\n+                # Default packages that all Lambda images need\n+                packages_to_hash = [\n+                    \"receipt_dynamo/receipt_dynamo\",\n+                    \"receipt_dynamo/pyproject.toml\",\n+                    \"receipt_chroma/receipt_chroma\",\n+                    \"receipt_chroma/pyproject.toml\",\n+                    \"receipt_label/receipt_label\",\n+                    \"receipt_label/pyproject.toml\",\n+                ]\n+\n+                # Add source_paths if specified (e.g., receipt_upload)\n+                if self.source_paths:\n+                    for source_path in self.source_paths:\n+                        packages_to_hash.append(source_path)\n+\n+                for package_path in packages_to_hash:\n+                    full_path = Path(PROJECT_DIR) / package_path\n+                    if full_path.exists():\n+                        paths.append(full_path)\n+\n+                # Also hash the handler directory\n+                handler_dir = (\n+                    Path(PROJECT_DIR) / Path(self.dockerfile_path).parent\n+                )\n+                if handler_dir.exists():\n+                    for file_path in sorted(handler_dir.rglob(\"*.py\")):\n+                        if file_path.is_file() and not any(\n+                            part.startswith(\".\") or part == \"__pycache__\"\n+                            for part in file_path.parts\n+                        ):\n+                            paths.append(file_path)","path":"infra/components/codebuild_docker_image.py","commit_id":"b4b5b64f14380322d789450da96a71cea99e1006","original_commit_id":"a305f6b5943e8ce8e40ab9a9d7ca3315da013746","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üü° Minor_\n\n**Dead code: inner `source_paths` check is unreachable.**\n\nThe `if self.source_paths:` block on line 232-234 is inside an `else` branch that only executes when `self.source_paths` is falsy (line 202). This inner check will never be true.\n\n\n```diff\n             if self.build_context_path == \".\":\n                 # Lambda images - hash Python packages and handler directory\n                 # Default packages that all Lambda images need\n                 packages_to_hash = [\n                     \"receipt_dynamo/receipt_dynamo\",\n                     \"receipt_dynamo/pyproject.toml\",\n                     \"receipt_chroma/receipt_chroma\",\n                     \"receipt_chroma/pyproject.toml\",\n                     \"receipt_label/receipt_label\",\n                     \"receipt_label/pyproject.toml\",\n                 ]\n\n-                # Add source_paths if specified (e.g., receipt_upload)\n-                if self.source_paths:\n-                    for source_path in self.source_paths:\n-                        packages_to_hash.append(source_path)\n-\n                 for package_path in packages_to_hash:\n```\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\ninfra/components/codebuild_docker_image.py around lines 217-251: the inner `if\nself.source_paths:` is unreachable because this branch only runs when\n`self.source_paths` is falsy; remove the redundant conditional and either (A)\nmove the source_paths handling out of this else so it runs when source_paths is\ntruthy, or (B) replace the inner check with a safe iteration that no-ops when\nsource_paths is None/empty (e.g., iterate over self.source_paths or an empty\nlist) so any provided source paths get appended to paths.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:ocelot -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commit 6ae1f43","created_at":"2025-12-06T03:01:34Z","updated_at":"2025-12-06T03:20:50Z","html_url":"https://github.com/tnorlund/Portfolio/pull/491#discussion_r2594458647","pull_request_url":"https://api.github.com/repos/tnorlund/Portfolio/pulls/491","_links":{"self":{"href":"https://api.github.com/repos/tnorlund/Portfolio/pulls/comments/2594458647"},"html":{"href":"https://github.com/tnorlund/Portfolio/pull/491#discussion_r2594458647"},"pull_request":{"href":"https://api.github.com/repos/tnorlund/Portfolio/pulls/491"}},"reactions":{"url":"https://api.github.com/repos/tnorlund/Portfolio/pulls/comments/2594458647/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":215,"original_start_line":217,"start_side":"RIGHT","line":244,"original_line":251,"side":"RIGHT","author_association":"NONE","original_position":251,"position":244,"subject_type":"line"},{"url":"https://api.github.com/repos/tnorlund/Portfolio/pulls/comments/2594458648","pull_request_review_id":3547057551,"id":2594458648,"node_id":"PRRC_kwDOM8BQvs6apEwY","diff_hunk":"@@ -0,0 +1,806 @@\n+#!/usr/bin/env python3\n+\"\"\"\n+ecs_lambda.py\n+\n+Hybrid Lambda Function component similar to lambda_layer.py, but for Lambda code.\n+\n+Goals:\n+- Fast `pulumi up` (async by default) by offloading packaging to AWS CodeBuild\n+- Simple architecture: S3 (source), CodePipeline ‚Üí CodeBuild ‚Üí update Lambda code\n+- Optionally wait in sync mode (useful in CI) to ensure function updated\n+- Supports including local `receipt-*` monorepo packages into the build\n+- Avoids Step Functions/SQS; keeps infra simple and debuggable\n+\n+Usage pattern mirrors `LambdaLayer` but targets function packaging and updates.\n+\"\"\"\n+\n+# pylint: disable=import-error\n+\n+import base64\n+import glob\n+import json\n+import os\n+import shlex\n+from typing import Any, Dict, List, Optional\n+\n+import pulumi\n+import pulumi_command as command\n+from pulumi import (\n+    AssetArchive,\n+    ComponentResource,\n+    Output,\n+    ResourceOptions,\n+    StringAsset,\n+)\n+from pulumi_aws import config, get_caller_identity\n+from pulumi_aws.codebuild import (\n+    Project,\n+    ProjectArtifactsArgs,\n+    ProjectEnvironmentArgs,\n+    ProjectEnvironmentEnvironmentVariableArgs,\n+    ProjectLogsConfigArgs,\n+    ProjectLogsConfigCloudwatchLogsArgs,\n+    ProjectSourceArgs,\n+)\n+from pulumi_aws.codepipeline import (\n+    Pipeline,\n+    PipelineArtifactStoreArgs,\n+    PipelineStageActionArgs,\n+    PipelineStageArgs,\n+)\n+from pulumi_aws.iam import Role as ROLE\n+from pulumi_aws.iam import RolePolicy, RolePolicyAttachment\n+from pulumi_aws.lambda_ import Function, FunctionEnvironmentArgs\n+\n+from infra.shared.build_utils import (\n+    compute_hash,\n+    make_artifact_bucket,\n+    make_log_group,\n+)\n+from infra.utils import _find_project_root\n+\n+PROJECT_DIR = _find_project_root()\n+\n+\n+class EcsLambda(ComponentResource):\n+    \"\"\"AWS-offloaded Lambda function builder/deployer using CodeBuild/CodePipeline.\n+\n+    This component handles:\n+    - Zipping your sources (with optional local monorepo deps) and uploading to S3\n+    - Running CodeBuild to assemble `function.zip` (pip installs, optional wheels)\n+    - Updating the Lambda code with `aws lambda update-function-code`\n+\n+    By default, `pulumi up` is fast (async). For CI/CD, enable sync to wait.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        name: str,\n+        *,\n+        package_dir: str,\n+        handler: str,\n+        python_version: str = \"3.12\",\n+        description: Optional[str] = None,\n+        role_arn: Optional[Output[str] | str] = None,\n+        timeout: int = 30,\n+        memory_size: int = 512,\n+        environment: Optional[Dict[str, str]] = None,\n+        layers: Optional[List[Output[str] | str]] = None,\n+        package_extras: Optional[\n+            str\n+        ] = None,  # e.g., \"lambda\" to install `pkg[lambda]`\n+        sync_mode: Optional[bool] = None,\n+        opts: Optional[ResourceOptions] = None,\n+    ) -> None:\n+        super().__init__(f\"ecs-lambda:{name}\", name, {}, opts)\n+\n+        self.name = name\n+        self.package_dir = package_dir\n+        self.handler = handler  # e.g., \"my_module.lambda_handler\"\n+        self.python_version = python_version\n+        self.description = (\n+            description or f\"AWS-built Lambda function for {name}\"\n+        )\n+        self.role_arn = role_arn\n+        self.timeout = timeout\n+        self.memory_size = memory_size\n+        self.environment = environment or {}\n+        self.layers = layers or []\n+        self.package_extras = package_extras\n+\n+        # Configure build mode\n+        if sync_mode is not None:\n+            self.sync_mode = sync_mode\n+        else:\n+            config = pulumi.Config(\"ecs-lambda\")\n+            if config.get_bool(\"sync-mode\"):\n+                self.sync_mode = True\n+            elif os.getenv(\"CI\") or os.getenv(\"GITHUB_ACTIONS\"):\n+                self.sync_mode = True\n+            else:\n+                self.sync_mode = False\n+\n+        # Additional config flags\n+        config = pulumi.Config(\"ecs-lambda\")\n+        self.force_rebuild = config.get_bool(\"force-rebuild\") or False\n+        self.debug_mode = config.get_bool(\"debug-mode\") or False","path":"infra/components/ecs_lambda.py","commit_id":"a305f6b5943e8ce8e40ab9a9d7ca3315da013746","original_commit_id":"a305f6b5943e8ce8e40ab9a9d7ca3315da013746","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Variable shadowing breaks AWS config access.**\n\nThe local `config` variable (lines 115, 124) shadows the imported `pulumi_aws.config` from line 35. Later in `_setup_pipeline`, `config.region` is used (lines 562, 567, 600, 612) expecting AWS region, but it will access the `pulumi.Config(\"ecs-lambda\")` object instead, which doesn't have a `region` attribute.\n\n\nRename the local config variables to avoid shadowing:\n\n```diff\n         # Configure build mode\n         if sync_mode is not None:\n             self.sync_mode = sync_mode\n         else:\n-            config = pulumi.Config(\"ecs-lambda\")\n-            if config.get_bool(\"sync-mode\"):\n+            ecs_config = pulumi.Config(\"ecs-lambda\")\n+            if ecs_config.get_bool(\"sync-mode\"):\n                 self.sync_mode = True\n             elif os.getenv(\"CI\") or os.getenv(\"GITHUB_ACTIONS\"):\n                 self.sync_mode = True\n             else:\n                 self.sync_mode = False\n\n         # Additional config flags\n-        config = pulumi.Config(\"ecs-lambda\")\n-        self.force_rebuild = config.get_bool(\"force-rebuild\") or False\n-        self.debug_mode = config.get_bool(\"debug-mode\") or False\n+        ecs_config = pulumi.Config(\"ecs-lambda\")\n+        self.force_rebuild = ecs_config.get_bool(\"force-rebuild\") or False\n+        self.debug_mode = ecs_config.get_bool(\"debug-mode\") or False\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n            ecs_config = pulumi.Config(\"ecs-lambda\")\n            if ecs_config.get_bool(\"sync-mode\"):\n                self.sync_mode = True\n            elif os.getenv(\"CI\") or os.getenv(\"GITHUB_ACTIONS\"):\n                self.sync_mode = True\n            else:\n                self.sync_mode = False\n\n        # Additional config flags\n        ecs_config = pulumi.Config(\"ecs-lambda\")\n        self.force_rebuild = ecs_config.get_bool(\"force-rebuild\") or False\n        self.debug_mode = ecs_config.get_bool(\"debug-mode\") or False\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn infra/components/ecs_lambda.py around lines 115 to 126, the local variable\nnamed \"config\" shadows the imported pulumi_aws.config and causes later calls\nlike config.region to reference the wrong object; rename the local\npulumi.Config(\"ecs-lambda\") instances (both the one used for sync-mode and the\nlater one for force_rebuild/debug_mode) to a distinct name such as\necs_lambda_config (or lambda_cfg) and update any local usages accordingly so the\nimported AWS config remains accessible as pulumi_aws.config; ensure both\noccurrences are renamed and no other local \"config\" variables remain that would\nshadow the import.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:ocelot -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commit 6ae1f43","created_at":"2025-12-06T03:01:34Z","updated_at":"2025-12-06T03:20:55Z","html_url":"https://github.com/tnorlund/Portfolio/pull/491#discussion_r2594458648","pull_request_url":"https://api.github.com/repos/tnorlund/Portfolio/pulls/491","_links":{"self":{"href":"https://api.github.com/repos/tnorlund/Portfolio/pulls/comments/2594458648"},"html":{"href":"https://github.com/tnorlund/Portfolio/pull/491#discussion_r2594458648"},"pull_request":{"href":"https://api.github.com/repos/tnorlund/Portfolio/pulls/491"}},"reactions":{"url":"https://api.github.com/repos/tnorlund/Portfolio/pulls/comments/2594458648/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":115,"start_side":"RIGHT","line":null,"original_line":126,"side":"RIGHT","author_association":"NONE","original_position":126,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/tnorlund/Portfolio/pulls/comments/2594458651","pull_request_review_id":3547057551,"id":2594458651,"node_id":"PRRC_kwDOM8BQvs6apEwb","diff_hunk":"@@ -0,0 +1,213 @@\n+\"\"\"Pulumi component for creating an EFS file system for shared data across instances.\"\"\"\n+\n+from typing import List, Optional\n+\n+import pulumi\n+import pulumi_aws as aws\n+from pulumi import ResourceOptions\n+\n+\n+class EFSStorage:\n+    \"\"\"Pulumi component for creating an EFS file system for shared data across instances.\"\"\"\n+\n+    def __init__(\n+        self,\n+        name: str,\n+        vpc_id: str,\n+        subnet_ids: List[str],\n+        security_group_ids: List[str],\n+        instance_role_name: str,\n+        performance_mode: str = \"generalPurpose\",\n+        throughput_mode: str = \"bursting\",\n+        encrypted: bool = True,\n+        lifecycle_policies: Optional[List[dict]] = None,\n+        opts: Optional[ResourceOptions] = None,\n+    ):","path":"infra/components/efs_storage.py","commit_id":"a305f6b5943e8ce8e40ab9a9d7ca3315da013746","original_commit_id":"a305f6b5943e8ce8e40ab9a9d7ca3315da013746","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**Class should inherit from `ComponentResource` for proper Pulumi integration.**\n\nThe `EFSStorage` class is described as a Pulumi component but doesn't inherit from `ComponentResource`. This means:\n- Resources won't be grouped together in Pulumi state\n- The `opts` parameter won't properly propagate parent relationships\n- No `register_outputs` for component outputs\n\nOther components in this PR (e.g., `LambdaFunctionsComponent`, `EcsLambda`) correctly inherit from `ComponentResource`.\n\n\n```diff\n-class EFSStorage:\n+class EFSStorage(pulumi.ComponentResource):\n     \"\"\"Pulumi component for creating an EFS file system for shared data across instances.\"\"\"\n\n     def __init__(\n         self,\n         name: str,\n         vpc_id: str,\n-        subnet_ids: List[str],\n+        subnet_ids: pulumi.Output[List[str]] | List[str],\n         security_group_ids: List[str],\n         instance_role_name: str,\n         performance_mode: str = \"generalPurpose\",\n         throughput_mode: str = \"bursting\",\n         encrypted: bool = True,\n         lifecycle_policies: Optional[List[dict]] = None,\n         opts: Optional[ResourceOptions] = None,\n     ):\n+        super().__init__(\"custom:efs:EFSStorage\", name, None, opts)\n```\n\nAnd at the end of `__init__`, add:\n```python\n        self.register_outputs({\n            \"file_system_id\": self.file_system_id,\n            \"dns_name\": self.file_system_dns_name,\n            \"training_access_point_id\": self.training_access_point_id,\n            \"checkpoints_access_point_id\": self.checkpoints_access_point_id,\n        })\n```\n\n<!-- fingerprinting:phantom:medusa:ocelot -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commit 6ae1f43","created_at":"2025-12-06T03:01:34Z","updated_at":"2025-12-06T03:20:59Z","html_url":"https://github.com/tnorlund/Portfolio/pull/491#discussion_r2594458651","pull_request_url":"https://api.github.com/repos/tnorlund/Portfolio/pulls/491","_links":{"self":{"href":"https://api.github.com/repos/tnorlund/Portfolio/pulls/comments/2594458651"},"html":{"href":"https://github.com/tnorlund/Portfolio/pull/491#discussion_r2594458651"},"pull_request":{"href":"https://api.github.com/repos/tnorlund/Portfolio/pulls/491"}},"reactions":{"url":"https://api.github.com/repos/tnorlund/Portfolio/pulls/comments/2594458651/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":10,"start_side":"RIGHT","line":null,"original_line":25,"side":"RIGHT","author_association":"NONE","original_position":25,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/tnorlund/Portfolio/pulls/comments/2594458652","pull_request_review_id":3547057551,"id":2594458652,"node_id":"PRRC_kwDOM8BQvs6apEwc","diff_hunk":"@@ -0,0 +1,213 @@\n+\"\"\"Pulumi component for creating an EFS file system for shared data across instances.\"\"\"\n+\n+from typing import List, Optional\n+\n+import pulumi\n+import pulumi_aws as aws\n+from pulumi import ResourceOptions\n+\n+\n+class EFSStorage:\n+    \"\"\"Pulumi component for creating an EFS file system for shared data across instances.\"\"\"\n+\n+    def __init__(\n+        self,\n+        name: str,\n+        vpc_id: str,\n+        subnet_ids: List[str],","path":"infra/components/efs_storage.py","commit_id":"a305f6b5943e8ce8e40ab9a9d7ca3315da013746","original_commit_id":"a305f6b5943e8ce8e40ab9a9d7ca3315da013746","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**Type annotation mismatch with runtime usage.**\n\nThe `subnet_ids` parameter is typed as `List[str]`, but on line 108 you call `subnet_ids.apply(create_mount_targets)`, which requires an `Output[List[str]]`. This will cause a runtime `AttributeError` if a plain list is passed.\n\nApply this diff to fix the type annotation:\n\n```diff\n-    subnet_ids: List[str],\n+    subnet_ids: pulumi.Output[List[str]] | List[str],\n```\n\nAlternatively, if subnet_ids should always be an Output, update the type and docstring accordingly:\n\n```diff\n-    subnet_ids: List[str],\n+    subnet_ids: pulumi.Output[List[str]],\n```\n\nAnd update the docstring on line 31:\n```diff\n-            subnet_ids: List of subnet IDs for mount targets\n+            subnet_ids: Output containing list of subnet IDs for mount targets\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        subnet_ids: pulumi.Output[List[str]] | List[str],\n```\n\n```suggestion\n        subnet_ids: pulumi.Output[List[str]],\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn infra/components/efs_storage.py around line 17 (and update the docstring at\nline 31 and usage at line 108), the subnet_ids parameter is annotated as\nList[str] but the code calls subnet_ids.apply(...), so change the type\nannotation to Output[List[str]] (import Output from pulumi) and update the\ndocstring to state subnet_ids is a Pulumi Output of a list of subnet IDs; ensure\nany callers pass an Output[List[str]] or wrap plain lists with\npulumi.Output.from_input to avoid AttributeError at runtime.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:ocelot -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commit 6ae1f43","created_at":"2025-12-06T03:01:34Z","updated_at":"2025-12-06T03:21:03Z","html_url":"https://github.com/tnorlund/Portfolio/pull/491#discussion_r2594458652","pull_request_url":"https://api.github.com/repos/tnorlund/Portfolio/pulls/491","_links":{"self":{"href":"https://api.github.com/repos/tnorlund/Portfolio/pulls/comments/2594458652"},"html":{"href":"https://github.com/tnorlund/Portfolio/pull/491#discussion_r2594458652"},"pull_request":{"href":"https://api.github.com/repos/tnorlund/Portfolio/pulls/491"}},"reactions":{"url":"https://api.github.com/repos/tnorlund/Portfolio/pulls/comments/2594458652/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":null,"original_line":17,"side":"RIGHT","author_association":"NONE","original_position":17,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/tnorlund/Portfolio/pulls/comments/2594458653","pull_request_review_id":3547057551,"id":2594458653,"node_id":"PRRC_kwDOM8BQvs6apEwd","diff_hunk":"@@ -0,0 +1,1485 @@\n+#!/usr/bin/env python3\n+\"\"\"\n+lambda_layer.py\n+\n+A hybrid Lambda Layer component that gives you the best of both worlds:\n+- Fast `pulumi up` for development (async builds with real ARNs)\n+- Simple architecture (no Step Functions/SQS complexity)\n+- Easy debugging and monitoring\n+\n+Modes:\n+- development (default): Fast `pulumi up`, builds happen in background\n+- sync: Wait for builds to complete (useful for CI/CD)\n+\"\"\"\n+\n+# pylint: disable=import-error\n+\n+import base64\n+import glob\n+import json\n+import os\n+import shlex\n+from typing import Any, Dict, List, Optional\n+\n+import pulumi\n+import pulumi_command as command\n+from pulumi import ComponentResource, Output\n+from pulumi_aws import config, get_caller_identity\n+from pulumi_aws.codebuild import (\n+    Project,\n+    ProjectArtifactsArgs,\n+    ProjectCacheArgs,\n+    ProjectEnvironmentArgs,\n+    ProjectEnvironmentEnvironmentVariableArgs,\n+    ProjectLogsConfigArgs,\n+    ProjectLogsConfigCloudwatchLogsArgs,\n+    ProjectSourceArgs,\n+)\n+from pulumi_aws.codepipeline import (\n+    Pipeline,\n+    PipelineArtifactStoreArgs,\n+    PipelineStageActionArgs,\n+    PipelineStageArgs,\n+)\n+from pulumi_aws.iam import Role as ROLE\n+from pulumi_aws.iam import RolePolicy\n+from pulumi_aws.lambda_ import LayerVersion\n+\n+from infra.shared.build_utils import (\n+    compute_hash,\n+    make_artifact_bucket,\n+    make_log_group,\n+    resolve_build_config,\n+)\n+from infra.shared.buildspecs import lambda_layer_buildspec\n+from infra.utils import _find_project_root\n+\n+PROJECT_DIR = _find_project_root()\n+# config will be initialized when needed in Pulumi context\n+\n+\n+class LambdaLayer(ComponentResource):\n+    \"\"\"\n+    A hybrid Lambda Layer component optimized for development speed.\n+\n+    Features:\n+    - Fast `pulumi up` (async builds by default)\n+    - Simple architecture (no Step Functions complexity)\n+    - Easy debugging with clear status\n+    - Configurable sync mode for CI/CD\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        name: str,\n+        package_dir: str,\n+        python_versions: List[str],\n+        description: Optional[str] = None,\n+        needs_pillow: bool = False,\n+        sync_mode: Optional[bool] = None,\n+        package_extras: Optional[\n+            str\n+        ] = None,  # e.g., \"lambda\" for receipt_label[lambda]\n+        opts: Optional[pulumi.ResourceOptions] = None,\n+    ):\n+        super().__init__(f\"lambda-layer:{name}\", name, {}, opts)\n+\n+        self.name = name\n+        self.layer_name = f\"{name}-{pulumi.get_stack()}\"\n+        self.package_dir = package_dir\n+\n+        # Accept either a single version string or a list\n+        if isinstance(python_versions, str):\n+            self.python_versions: List[str] = [python_versions]\n+        else:\n+            self.python_versions = list(python_versions)\n+\n+        self.description = (\n+            description or f\"Automatically built Lambda layer for {name}\"\n+        )\n+        self.needs_pillow = needs_pillow\n+        self.package_extras = package_extras\n+        self.opts = opts\n+\n+        # Determine build mode\n+        # Priority: parameter > config > CI detection > default (async)\n+        (\n+            self.sync_mode,\n+            self.force_rebuild,\n+            self.debug_mode,\n+        ) = resolve_build_config(\n+            \"lambda-layer\", sync_override=sync_mode, ci_default_sync=True\n+        )\n+\n+        # Validate package directory\n+        self._validate_package_dir()\n+\n+        # Calculate package hash for change detection\n+        package_hash = self._calculate_package_hash()\n+        package_path = os.path.join(PROJECT_DIR, self.package_dir)\n+\n+        # Show build mode and change detection info\n+        if self.sync_mode:\n+            pulumi.log.info(\n+                f\"üîÑ Building layer '{self.name}' in SYNC mode (will wait for completion)\"\n+            )\n+        else:\n+            pulumi.log.info(\n+                f\"‚ö° Layer '{self.name}' in ASYNC mode (fast pulumi up)\"\n+            )\n+            if self.force_rebuild:\n+                pulumi.log.info(\n+                    \"   üî® Force rebuild enabled - will trigger build\"\n+                )\n+            else:\n+                pulumi.log.info(\n+                    f\"   üì¶ Hash: {package_hash[:12]}... - will build only if changed\"\n+                )\n+\n+        self._setup_fast_build(package_hash, package_path)\n+\n+    def _validate_package_dir(self) -> None:\n+        \"\"\"Validate that the package directory exists and contains the necessary files.\"\"\"\n+        package_path = os.path.join(PROJECT_DIR, self.package_dir)\n+\n+        if not os.path.exists(package_path):\n+            raise ValueError(\n+                f\"Package directory {package_path} does not exist\"\n+            )\n+\n+        required_files = [\"pyproject.toml\"]\n+        missing_files = [\n+            f\n+            for f in required_files\n+            if not os.path.exists(os.path.join(package_path, f))\n+        ]\n+        if missing_files:\n+            raise ValueError(\n+                f\"Package directory {package_path} is missing required files: \"\n+                f\"{', '.join(missing_files)}\"\n+            )\n+\n+        python_files = glob.glob(\n+            os.path.join(package_path, \"**/*.py\"), recursive=True\n+        )\n+        if not python_files:\n+            raise ValueError(\n+                f\"Package directory {package_path} contains no Python files\"\n+            )\n+\n+    def _calculate_package_hash(self) -> str:\n+        \"\"\"Calculate a hash of the package contents to detect changes.\"\"\"\n+        package_path = os.path.join(PROJECT_DIR, self.package_dir)\n+        return compute_hash(\n+            [package_path],\n+            include_globs=[\"**/*.py\", \"pyproject.toml\"],\n+            extra_strings={\"package_dir\": self.package_dir},\n+        )\n+\n+    def _get_local_dependencies(self) -> List[str]:\n+        \"\"\"Get list of local package dependencies from pyproject.toml.\"\"\"\n+        package_path = os.path.join(PROJECT_DIR, self.package_dir)\n+        pyproject_path = os.path.join(package_path, \"pyproject.toml\")\n+\n+        local_deps = []\n+        if os.path.exists(pyproject_path):\n+            try:\n+                # Try Python 3.11+ built-in tomllib first\n+                try:\n+                    import tomllib\n+\n+                    with open(pyproject_path, \"rb\") as f:\n+                        data = tomllib.load(f)\n+                except ImportError:\n+                    # Fall back to toml package if available\n+                    try:\n+                        import toml\n+\n+                        with open(pyproject_path, \"r\") as f:\n+                            data = toml.load(f)\n+                    except ImportError:\n+                        # If neither is available, parse manually for basic dependencies\n+                        pulumi.log.warn(\n+                            f\"TOML parser not available, using basic parsing for {self.name}\"\n+                        )\n+                        data = self._parse_pyproject_basic(pyproject_path)\n+\n+                # Check main dependencies\n+                deps = data.get(\"project\", {}).get(\"dependencies\", [])\n+\n+                # Also check optional dependencies if using extras\n+                if self.package_extras:\n+                    optional_deps = data.get(\"project\", {}).get(\n+                        \"optional-dependencies\", {}\n+                    )\n+                    if self.package_extras in optional_deps:\n+                        deps.extend(optional_deps[self.package_extras])\n+\n+                # Filter for local packages (receipt-*)\n+                for dep in deps:\n+                    # Extract package name from version spec\n+                    dep_name = (\n+                        dep.split(\"[\")[0]\n+                        .split(\">\")[0]\n+                        .split(\"<\")[0]\n+                        .split(\"=\")[0]\n+                        .strip()\n+                    )\n+                    if dep_name.startswith(\"receipt-\"):\n+                        # Convert package name to directory name (receipt-dynamo -> receipt_dynamo)\n+                        dir_name = dep_name.replace(\"-\", \"_\")\n+                        local_path = os.path.join(PROJECT_DIR, dir_name)\n+                        if os.path.exists(local_path):\n+                            local_deps.append(dir_name)\n+                            pulumi.log.info(\n+                                f\"üì¶ Found local dependency: {dir_name} for {self.name}\"\n+                            )\n+            except Exception as e:\n+                pulumi.log.warn(f\"Could not parse pyproject.toml: {e}\")\n+\n+        return local_deps\n+\n+    def _parse_pyproject_basic(self, pyproject_path: str) -> Dict[str, Any]:\n+        \"\"\"\n+        Basic parser for pyproject.toml to extract dependencies when toml module\n+        is not available.\n+        \"\"\"\n+        dependencies: List[str] = []\n+        optional_deps: Dict[str, List[str]] = {}\n+        result: Dict[str, Any] = {\n+            \"project\": {\n+                \"dependencies\": dependencies,\n+                \"optional-dependencies\": optional_deps,\n+            }\n+        }\n+\n+        try:\n+            with open(pyproject_path, \"r\") as f:\n+                lines = f.readlines()\n+\n+            in_dependencies = False\n+            in_optional = False\n+            current_extra = None\n+\n+            for line in lines:\n+                line = line.strip()\n+\n+                # Start of dependencies section\n+                if line == \"dependencies = [\":\n+                    in_dependencies = True\n+                    continue\n+\n+                # End of dependencies section\n+                if in_dependencies and line == \"]\":\n+                    in_dependencies = False\n+                    continue\n+\n+                # Check for optional dependencies\n+                if \"[project.optional-dependencies]\" in line:\n+                    in_optional = True\n+                    continue\n+\n+                # Parse optional dependency sections\n+                if in_optional and \"= [\" in line:\n+                    current_extra = line.split(\"=\")[0].strip()\n+                    optional_deps[current_extra] = []\n+                    continue\n+\n+                # End of optional section\n+                if in_optional and line == \"]\":\n+                    current_extra = None\n+                    continue\n+\n+                # Parse dependency lines\n+                if in_dependencies and line and line != \"]\":\n+                    # Remove quotes and commas\n+                    dep = line.strip(\" \\\",'\")\n+                    if dep and not dep.startswith(\"#\"):\n+                        dependencies.append(dep)\n+\n+                # Parse optional dependency lines\n+                if current_extra and line and line != \"]\":\n+                    dep = line.strip(\" \\\",'\")\n+                    if dep and not dep.startswith(\"#\") and current_extra:\n+                        optional_deps[current_extra].append(dep)\n+\n+        except Exception as e:\n+            pulumi.log.warn(f\"Basic parsing failed: {e}\")\n+\n+        return result\n+\n+    def _encode_shell_script(self, script_content: str) -> str:\n+        \"\"\"Encode a shell script to base64 for use in buildspec to avoid parsing issues.\"\"\"\n+        return base64.b64encode(script_content.encode(\"utf-8\")).decode(\"utf-8\")\n+\n+    def _generate_batched_pip_install(\n+        self,\n+        packages: List[str],\n+        target_dir: str,\n+        python_version: str,\n+        batch_size: int = 10,\n+    ) -> List[str]:\n+        \"\"\"Generate batched pip install commands to avoid ARG_MAX errors.\n+\n+        Args:\n+            packages: List of package names to install\n+            target_dir: Target directory for installation\n+            python_version: Python version to use\n+            batch_size: Number of packages per batch\n+\n+        Returns:\n+            List of pip install commands\n+        \"\"\"\n+        if not packages:\n+            return []\n+\n+        commands = []\n+        for i in range(0, len(packages), batch_size):\n+            batch = packages[i : i + batch_size]\n+            pkg_str = \" \".join(batch)\n+            commands.append(\n+                f\"python{python_version} -m pip install --no-cache-dir --no-compile \"\n+                f\"{pkg_str} -t {target_dir}\"\n+            )\n+        return commands\n+\n+    def _get_update_functions_script(self) -> str:\n+        \"\"\"Generate the shell script for updating Lambda functions.\"\"\"\n+        return '''#!/bin/bash\n+set -e\n+LAYER_BASE_ARN=$(echo \"$NEW_LAYER_ARN\" | sed \"s/:[^:]*$//\")\n+\n+# Function to update a single Lambda function (for parallel execution)\n+update_function() {\n+  local FUNC_NAME=\"$1\"\n+  local FUNC_ARN=\"$2\"\n+\n+  echo \"Checking function: $FUNC_NAME\"\n+  ENV_TAG=$(aws lambda list-tags --resource \"$FUNC_ARN\" \\\n+    --query \"Tags.environment\" --output text 2>/dev/null || echo \"None\")\n+\n+  if [ \"$ENV_TAG\" != \"$STACK_NAME\" ]; then\n+    echo \"  Skipping $FUNC_NAME (environment: $ENV_TAG)\"\n+    return 0\n+  fi\n+\n+  echo \"  Function $FUNC_NAME matches environment $STACK_NAME\"\n+  CURRENT_LAYERS=$(aws lambda get-function-configuration --function-name \"$FUNC_NAME\" \\\n+    --query \"Layers[*].Arn\" --output text)\n+\n+  # Quick check: does this function actually use the layer we are updating?\n+  if [ -z \"$CURRENT_LAYERS\" ] || [ \"$CURRENT_LAYERS\" = \"None\" ]; then\n+    echo \"  Skipping $FUNC_NAME (no layers)\"\n+    return 0\n+  fi\n+\n+  # Check if function uses the layer being updated\n+  USES_LAYER=false\n+  for LAYER in $CURRENT_LAYERS; do\n+    LAYER_BASE=$(echo \"$LAYER\" | sed \"s/:[^:]*$//\")\n+    if [ \"$LAYER_BASE\" = \"$LAYER_BASE_ARN\" ]; then\n+      USES_LAYER=true\n+      break\n+    fi\n+  done\n+\n+  if [ \"$USES_LAYER\" = \"false\" ]; then\n+    echo \"  Skipping $FUNC_NAME (does not use this layer)\"\n+    return 0\n+  fi\n+\n+  # Build new layer list\n+  NEW_LAYERS=\"\"\n+  for LAYER in $CURRENT_LAYERS; do\n+    LAYER_BASE=$(echo \"$LAYER\" | sed \"s/:[^:]*$//\")\n+    LAYER_NAME=$(echo \"$LAYER_BASE\" | sed \"s/.*://\")\n+\n+    if [ \"$LAYER_BASE\" = \"$LAYER_BASE_ARN\" ]; then\n+      echo \"    Replacing old version: $LAYER\"\n+      continue\n+    fi\n+\n+    if echo \"$LAYER_NAME\" | grep -q \"\\\\-$STACK_NAME$\"; then\n+      NEW_LAYERS=\"$NEW_LAYERS $LAYER\"\n+      echo \"    Keeping env-specific layer: $LAYER\"\n+    elif echo \"$LAYER_BASE\" | grep -q \"\\\\-$STACK_NAME:\"; then\n+      NEW_LAYERS=\"$NEW_LAYERS $LAYER\"\n+      echo \"    Keeping env-specific layer: $LAYER\"\n+    else\n+      NEW_LAYERS=\"$NEW_LAYERS $LAYER\"\n+      BASE_LAYER_NAME=$(echo \"$LAYER_NAME\" | sed \"s/\\\\-[^\\\\-]*$//\")\n+  echo \"    Keeping cross-env layer: $LAYER \"\n+  echo \"      (consider migrating to ${BASE_LAYER_NAME}-$STACK_NAME)\"\n+    fi\n+  done\n+\n+  NEW_LAYERS=\"$NEW_LAYERS $NEW_LAYER_ARN\"\n+  NEW_LAYERS=$(echo \"$NEW_LAYERS\" | xargs)\n+  echo \"  Updating $FUNC_NAME with layers: $NEW_LAYERS\"\n+\n+  if aws lambda update-function-configuration --function-name \"$FUNC_NAME\" \\\n+    --layers $NEW_LAYERS >/dev/null 2>&1; then\n+    echo \"  ‚úÖ Updated $FUNC_NAME successfully\"\n+  else\n+    echo \"  ‚ùå Failed to update $FUNC_NAME\"\n+  fi\n+}\n+\n+# Export function for parallel execution\n+export -f update_function\n+export STACK_NAME LAYER_BASE_ARN NEW_LAYER_ARN\n+\n+echo \"üîç Finding Lambda functions that use this layer...\"\n+\n+# Get functions and filter in parallel (max 10 concurrent updates)\n+aws lambda list-functions --query \"Functions[*].[FunctionName,FunctionArn]\" --output text | \\\n+  grep -v \"^None\" | \\\n+  xargs -n 2 -P 10 bash -c 'update_function \"$@\"' _\n+\n+echo \"üéâ Parallel function updates completed!\"'''\n+\n+    def _get_buildspec(self, version: str | None = None) -> Dict[str, Any]:\n+        \"\"\"Return a buildspec dict for CodeBuild.\"\"\"\n+        return lambda_layer_buildspec(\n+            version=version,\n+            python_versions=self.python_versions,\n+            package_extras=self.package_extras,\n+            needs_pillow=self.needs_pillow,\n+            package_name=self.name,\n+            layer_name=self.layer_name,\n+            debug_mode=self.debug_mode,\n+        )\n+\n+    def _setup_fast_build(self, package_hash: str, package_path: str) -> None:\n+        \"\"\"Set up the fast build process with CodePipeline and per-version CodeBuild projects.\"\"\"\n+\n+        # Create S3 bucket for artifacts - let Pulumi auto-generate unique name\n+        build_bucket, _bucket_versioning = make_artifact_bucket(\n+            self.name, parent=self\n+        )\n+\n+        # Upload source command (runs on create and update, triggers on package_hash)\n+        upload_cmd = command.local.Command(\n+            f\"{self.name}-upload-source\",\n+            create=build_bucket.bucket.apply(\n+                lambda b: self._create_and_run_upload_script(\n+                    b, package_path, package_hash\n+                )\n+            ),\n+            update=build_bucket.bucket.apply(\n+                lambda b: self._create_and_run_upload_script(\n+                    b, package_path, package_hash\n+                )\n+            ),\n+            triggers=[package_hash],\n+            opts=pulumi.ResourceOptions(\n+                parent=self,\n+                delete_before_replace=True,\n+            ),\n+        )\n+\n+        # Create IAM role for CodeBuild/CodePipeline\n+        codebuild_role = ROLE(\n+            f\"{self.name}-codebuild-role\",\n+            assume_role_policy=json.dumps(\n+                {\n+                    \"Version\": \"2012-10-17\",\n+                    \"Statement\": [\n+                        {\n+                            \"Effect\": \"Allow\",\n+                            \"Principal\": {\n+                                \"Service\": \"codebuild.amazonaws.com\"\n+                            },\n+                            \"Action\": \"sts:AssumeRole\",\n+                        },\n+                        {\n+                            \"Effect\": \"Allow\",\n+                            \"Principal\": {\n+                                \"Service\": \"codepipeline.amazonaws.com\"\n+                            },\n+                            \"Action\": \"sts:AssumeRole\",\n+                        },\n+                    ],\n+                }\n+            ),\n+            opts=pulumi.ResourceOptions(parent=self),\n+        )\n+\n+        # Create CodeBuild policy with permissions for layer publishing and function updates\n+        RolePolicy(\n+            f\"{self.name}-codebuild-policy\",\n+            role=codebuild_role.id,\n+            policy=pulumi.Output.all(build_bucket.arn, self.layer_name).apply(\n+                lambda args: json.dumps(\n+                    {\n+                        \"Version\": \"2012-10-17\",\n+                        \"Statement\": [\n+                            {\n+                                \"Effect\": \"Allow\",\n+                                \"Action\": [\n+                                    \"logs:CreateLogGroup\",\n+                                    \"logs:CreateLogStream\",\n+                                    \"logs:PutLogEvents\",\n+                                ],\n+                                \"Resource\": [\n+                                    (\n+                                        f\"arn:aws:logs:{config.region}:\"\n+                                        f\"{get_caller_identity().account_id}:\"\n+                                        \"log-group:/aws/codebuild/*\"\n+                                    ),\n+                                    (\n+                                        f\"arn:aws:logs:{config.region}:\"\n+                                        f\"{get_caller_identity().account_id}:\"\n+                                        \"log-group:/aws/codebuild/*:*\"\n+                                    ),\n+                                ],\n+                            },\n+                            {\n+                                \"Effect\": \"Allow\",\n+                                \"Action\": [\n+                                    \"s3:GetObject\",\n+                                    \"s3:GetObjectVersion\",\n+                                    \"s3:PutObject\",\n+                                ],\n+                                \"Resource\": f\"{args[0]}/*\",\n+                            },\n+                            {\n+                                \"Effect\": \"Allow\",\n+                                \"Action\": [\n+                                    \"s3:GetBucketAcl\",\n+                                    \"s3:GetBucketLocation\",\n+                                    \"s3:ListBucket\",\n+                                ],\n+                                \"Resource\": args[0],\n+                            },\n+                            {\n+                                \"Effect\": \"Allow\",\n+                                \"Action\": [\"lambda:PublishLayerVersion\"],\n+                                \"Resource\": [\n+                                    f\"arn:aws:lambda:*:*:layer:{args[1]}\",\n+                                    f\"arn:aws:lambda:*:*:layer:{args[1]}:*\",\n+                                ],\n+                            },\n+                            {\n+                                \"Effect\": \"Allow\",\n+                                \"Action\": [\n+                                    \"lambda:UpdateFunctionConfiguration\",\n+                                    \"lambda:ListFunctions\",\n+                                    \"lambda:ListTags\",\n+                                    \"lambda:GetFunctionConfiguration\",\n+                                    \"lambda:GetLayerVersion\",\n+                                ],\n+                                \"Resource\": \"*\",\n+                            },\n+                            {\n+                                \"Effect\": \"Allow\",\n+                                \"Action\": [\n+                                    \"codebuild:StartBuild\",\n+                                    \"codebuild:StartBuildBatch\",\n+                                    \"codebuild:BatchGetBuilds\",\n+                                    \"codebuild:BatchGetBuildBatches\",\n+                                ],\n+                                \"Resource\": (\n+                                    f\"arn:aws:codebuild:{config.region}:\"\n+                                    f\"{get_caller_identity().account_id}:project/*\"\n+                                ),\n+                            },\n+                        ],\n+                    }\n+                )\n+            ),\n+            opts=pulumi.ResourceOptions(parent=self),\n+        )\n+\n+        # Create IAM role for CodePipeline\n+        pipeline_role = ROLE(\n+            f\"{self.name}-pipeline-role\",\n+            assume_role_policy=json.dumps(\n+                {\n+                    \"Version\": \"2012-10-17\",\n+                    \"Statement\": [\n+                        {\n+                            \"Effect\": \"Allow\",\n+                            \"Principal\": {\n+                                \"Service\": \"codepipeline.amazonaws.com\"\n+                            },\n+                            \"Action\": \"sts:AssumeRole\",\n+                        }\n+                    ],\n+                }\n+            ),\n+            opts=pulumi.ResourceOptions(parent=self),\n+        )\n+\n+        # Grant CodePipeline read/write access to the S3 artifact bucket\n+        RolePolicy(\n+            f\"{self.name}-pipeline-s3-policy\",\n+            role=pipeline_role.id,\n+            policy=pulumi.Output.all(build_bucket.arn).apply(\n+                lambda args: json.dumps(\n+                    {\n+                        \"Version\": \"2012-10-17\",\n+                        \"Statement\": [\n+                            # 1) Bucket-level read/list\n+                            {\n+                                \"Effect\": \"Allow\",\n+                                \"Action\": [\n+                                    \"s3:ListBucket\",\n+                                    \"s3:GetBucketLocation\",\n+                                    \"s3:GetBucketVersioning\",\n+                                    \"s3:GetBucketAcl\",\n+                                    \"s3:GetBucketPolicy\",\n+                                    \"s3:GetBucketPublicAccessBlock\",\n+                                ],\n+                                \"Resource\": args[0],\n+                            },\n+                            # 2) Read your source objects\n+                            {\n+                                \"Effect\": \"Allow\",\n+                                \"Action\": [\n+                                    \"s3:GetObject\",\n+                                    \"s3:GetObjectVersion\",\n+                                    \"s3:GetObjectAcl\",\n+                                    \"s3:GetObjectVersionAcl\",\n+                                    \"s3:GetObjectTagging\",\n+                                    \"s3:GetObjectVersionTagging\",\n+                                ],\n+                                \"Resource\": f\"{args[0]}/{self.name}/*\",\n+                            },\n+                            # 3) Write pipeline artifacts anywhere in the bucket\n+                            {\n+                                \"Effect\": \"Allow\",\n+                                \"Action\": [\"s3:PutObject\"],\n+                                \"Resource\": f\"{args[0]}/*\",\n+                            },\n+                        ],\n+                    }\n+                )\n+            ),\n+            opts=pulumi.ResourceOptions(parent=self),\n+        )\n+\n+        # Grant CodePipeline permission to invoke CodeBuild projects (including StartBuildBatch)\n+        RolePolicy(\n+            f\"{self.name}-pipeline-codebuild-policy\",\n+            role=pipeline_role.id,\n+            policy=Output.all(\n+                config.region, get_caller_identity().account_id\n+            ).apply(\n+                lambda args: json.dumps(\n+                    {\n+                        \"Version\": \"2012-10-17\",\n+                        \"Statement\": [\n+                            {\n+                                \"Effect\": \"Allow\",\n+                                \"Action\": [\n+                                    \"codebuild:StartBuild\",\n+                                    \"codebuild:StartBuildBatch\",\n+                                    \"codebuild:BatchGetBuilds\",\n+                                    \"codebuild:BatchGetBuildBatches\",\n+                                    \"codebuild:BatchGetProjects\",\n+                                    \"codebuild:ListBuildsForProject\",\n+                                ],\n+                                \"Resource\": (\n+                                    f\"arn:aws:codebuild:{args[0]}:{args[1]}:project/*\"\n+                                ),\n+                            }\n+                        ],\n+                    }\n+                )\n+            ),\n+            opts=pulumi.ResourceOptions(parent=self),\n+        )\n+\n+        # Create a CodeBuild project for each Python version\n+        build_projects = {}\n+        stack_name = pulumi.get_stack()\n+        for v in self.python_versions:\n+            project = Project(\n+                # Pulumi logical name with stack\n+                f\"{self.name}-build-py{v.replace('.', '')}-{stack_name}\",\n+                service_role=codebuild_role.arn,\n+                source=ProjectSourceArgs(\n+                    type=\"S3\",\n+                    # instead of lambda b: f\"{self.name}/source.zip\", do:\n+                    location=build_bucket.bucket.apply(\n+                        lambda b: f\"{b}/{self.name}/source.zip\"\n+                    ),\n+                    buildspec=json.dumps(self._get_buildspec(version=v)),\n+                ),\n+                artifacts=ProjectArtifactsArgs(\n+                    type=\"S3\",\n+                    location=build_bucket.bucket,\n+                    path=f\"{self.name}/py{v.replace('.', '')}\",\n+                    name=\"layer.zip\",\n+                    packaging=\"ZIP\",\n+                    namespace_type=\"NONE\",\n+                ),\n+                environment=ProjectEnvironmentArgs(\n+                    type=\"ARM_CONTAINER\",\n+                    compute_type=\"BUILD_GENERAL1_SMALL\",\n+                    image=f\"aws/codebuild/amazonlinux-aarch64-standard:3.0\",\n+                    environment_variables=[\n+                        ProjectEnvironmentEnvironmentVariableArgs(\n+                            name=\"PYTHON_VERSION\", value=v\n+                        ),\n+                        ProjectEnvironmentEnvironmentVariableArgs(\n+                            name=\"LAYER_NAME\", value=self.layer_name\n+                        ),\n+                        ProjectEnvironmentEnvironmentVariableArgs(\n+                            name=\"PACKAGE_NAME\", value=self.name\n+                        ),\n+                        ProjectEnvironmentEnvironmentVariableArgs(\n+                            name=\"BUCKET_NAME\", value=build_bucket.bucket\n+                        ),\n+                        ProjectEnvironmentEnvironmentVariableArgs(\n+                            name=\"PACKAGE_DIR\", value=\"source\"\n+                        ),\n+                        ProjectEnvironmentEnvironmentVariableArgs(\n+                            name=\"STACK_NAME\", value=pulumi.get_stack()\n+                        ),\n+                        ProjectEnvironmentEnvironmentVariableArgs(\n+                            name=\"NEEDS_PILLOW\", value=str(self.needs_pillow)\n+                        ),\n+                        ProjectEnvironmentEnvironmentVariableArgs(\n+                            name=\"DEBUG_MODE\", value=str(self.debug_mode)\n+                        ),\n+                    ],\n+                ),\n+                cache=ProjectCacheArgs(\n+                    type=\"S3\",\n+                    location=build_bucket.bucket.apply(\n+                        lambda b: f\"{b}/{self.name}/cache\"\n+                    ),\n+                ),\n+                logs_config=ProjectLogsConfigArgs(\n+                    cloudwatch_logs=ProjectLogsConfigCloudwatchLogsArgs(\n+                        status=\"ENABLED\",\n+                        # Auto-generate log group name\n+                    ),\n+                ),\n+                opts=pulumi.ResourceOptions(parent=self),\n+            )\n+            build_projects[v] = project\n+\n+        def publish_buildspec() -> Dict[str, Any]:\n+            # This buildspec merges all version artifacts under a single\n+            # python/lib/python<ver>/site-packages tree, zips, uploads to S3,\n+            # and publishes as one layer from S3.\n+            commands = []\n+            # Step 1: Prepare merged directory\n+            commands.append('echo \"Preparing merged layer directory...\"')\n+            commands.append(\"rm -rf merged && mkdir -p merged\")\n+            # Step 2: Merge already-flattened artifacts (build stage already flattened to python/*)\n+            commands.append(\n+                'echo \"Setting up merged python directory (already flattened)...\"'\n+            )\n+            commands.append(\"rm -rf merged/python && mkdir -p merged/python\")\n+            for idx, v in enumerate(self.python_versions):\n+                commands.append(\n+                    f'echo \"Merging flattened artifacts for Python {v}...\"'\n+                )\n+                if idx == 0:\n+                    # Primary artifact in root workspace - already flattened to python/*\n+                    commands.append(\"cp -r python/* merged/python/\")\n+                else:\n+                    # Secondary artifacts under CODEBUILD_SRC_DIR_py<ver> (ver without dots)\n+                    # These are also already flattened to python/*\n+                    ver = v.replace(\".\", \"\")\n+                    commands.append(\n+                        f\"cp -r $CODEBUILD_SRC_DIR_py{ver}/python/* merged/python/\"\n+                    )\n+            # Validate the flattened structure before zipping\n+            commands.append('echo \"Validating flattened structure...\"')\n+            commands.append(\n+                'if [ -d \"merged/python/lib\" ]; then '\n+                'echo \"ERROR: Nested lib directory found! Layer should be flattened.\"; '\n+                \"exit 1; fi\"\n+            )\n+            commands.append(\n+                'echo \"Structure is correctly flattened (no nested lib directory)\"'\n+            )\n+            # Step 3: Zip the merged python directory\n+            commands.append('echo \"Zipping merged layer...\"')\n+            commands.append(\"cd merged && zip -r ../layer.zip python && cd ..\")\n+            # Validate the zip file\n+            commands.append('echo \"Validating layer.zip...\"')\n+            commands.append(\n+                \"[ -f layer.zip ] || { echo 'ERROR: layer.zip not created'; exit 1; }\"\n+            )\n+            commands.append(\n+                \"ZIP_SIZE=$(stat -c%s layer.zip 2>/dev/null || \"\n+                \"stat -f%z layer.zip 2>/dev/null || echo 0)\"\n+            )\n+            commands.append('echo \"Layer zip size: $ZIP_SIZE bytes\"')\n+            commands.append(\n+                \"[ \\\"$ZIP_SIZE\\\" -gt 0 ] || { echo 'ERROR: layer.zip is empty'; exit 1; }\"\n+            )\n+            # Check if zip size exceeds Lambda limits\n+            commands.append(\n+                \"MAX_SIZE=$((250 * 1024 * 1024))  # 250MB in bytes\"\n+            )\n+            commands.append(\n+                'if [ \"$ZIP_SIZE\" -gt \"$MAX_SIZE\" ]; then '\n+                'echo \"WARNING: Layer size exceeds Lambda limit (250MB)\"; '\n+                'echo \"Size: $(($ZIP_SIZE / 1024 / 1024))MB\"; fi'\n+            )\n+            # Step 3.1: Upload combined zip to artifact bucket\n+            commands.append('echo \"Uploading merged layer.zip to S3...\"')\n+            commands.append(\n+                \"aws s3 cp layer.zip s3://$BUCKET_NAME/${PACKAGE_NAME}/combined/layer.zip\"\n+            )\n+            # Step 4: Publish the merged layer from S3\n+            commands.append(\n+                'echo \"Publishing merged layer from S3 to Lambda...\"'\n+            )\n+            commands.append(\n+                \"NEW_LAYER_ARN=$(aws lambda publish-layer-version \"\n+                '--layer-name \"$LAYER_NAME\" '\n+                '--content S3Bucket=\"$BUCKET_NAME\",S3Key=\"${PACKAGE_NAME}/combined/layer.zip\" '\n+                \"--compatible-runtimes \"\n+                + \" \".join([f\"python{v}\" for v in self.python_versions])\n+                + \" --compatible-architectures arm64 \"\n+                f'--description \"{self.description}\" '\n+                '--query \"LayerVersionArn\" --output text 2>&1) || '\n+                '{ echo \"ERROR: Failed to publish layer version\"; '\n+                'echo \"Output: $NEW_LAYER_ARN\"; exit 1; }'\n+            )\n+            commands.append('echo \"New layer ARN: $NEW_LAYER_ARN\"')\n+            commands.append(\n+                \"[ -n \\\"$NEW_LAYER_ARN\\\" ] || { echo 'ERROR: No layer ARN returned'; exit 1; }\"\n+            )\n+            commands.append(\"export NEW_LAYER_ARN\")\n+            commands.append(\n+                f'echo \"{self._encode_shell_script(self._get_update_functions_script())}\" '\n+                \"| base64 -d > update_layers.sh\"\n+            )\n+            commands.append(\"chmod +x update_layers.sh\")\n+            commands.append(\"./update_layers.sh\")\n+            commands.append(\n+                'echo \"All layer versions published and functions updated.\"'\n+            )\n+            return {\n+                \"version\": 0.2,\n+                \"phases\": {\n+                    \"build\": {\n+                        \"commands\": commands,\n+                    }\n+                },\n+            }\n+\n+        publish_log_group = make_log_group(\n+            f\"{self.name}-publish-logs\",\n+            retention_days=14,\n+            parent=self,\n+        )\n+\n+        # Create the publish CodeBuild project\n+        publish_project = Project(\n+            f\"{self.name}-publish-{stack_name}\",  # Pulumi logical name with stack\n+            service_role=codebuild_role.arn,\n+            source=ProjectSourceArgs(\n+                type=\"CODEPIPELINE\",\n+                buildspec=json.dumps(publish_buildspec()),\n+            ),\n+            artifacts=ProjectArtifactsArgs(type=\"CODEPIPELINE\"),\n+            environment=ProjectEnvironmentArgs(\n+                type=\"ARM_CONTAINER\",\n+                compute_type=\"BUILD_GENERAL1_SMALL\",\n+                image=\"aws/codebuild/amazonlinux-aarch64-standard:3.0\",\n+                environment_variables=[\n+                    ProjectEnvironmentEnvironmentVariableArgs(\n+                        name=\"LAYER_NAME\", value=self.layer_name\n+                    ),\n+                    ProjectEnvironmentEnvironmentVariableArgs(\n+                        name=\"PACKAGE_NAME\", value=self.name\n+                    ),\n+                    ProjectEnvironmentEnvironmentVariableArgs(\n+                        name=\"BUCKET_NAME\", value=build_bucket.bucket\n+                    ),\n+                    ProjectEnvironmentEnvironmentVariableArgs(\n+                        name=\"STACK_NAME\", value=pulumi.get_stack()\n+                    ),\n+                    ProjectEnvironmentEnvironmentVariableArgs(\n+                        name=\"NEEDS_PILLOW\", value=str(self.needs_pillow)\n+                    ),\n+                    ProjectEnvironmentEnvironmentVariableArgs(\n+                        name=\"DEBUG_MODE\", value=str(self.debug_mode)\n+                    ),\n+                ],\n+            ),\n+            build_timeout=60,\n+            logs_config=ProjectLogsConfigArgs(\n+                cloudwatch_logs=ProjectLogsConfigCloudwatchLogsArgs(\n+                    status=\"ENABLED\",\n+                    group_name=publish_log_group.name,\n+                ),\n+            ),\n+            opts=pulumi.ResourceOptions(parent=self),\n+        )\n+\n+        # Define CodePipeline to run all builds in parallel and then publish layer versions\n+        pipeline = Pipeline(\n+            f\"{self.name}-pipeline-{stack_name}\",  # Pulumi logical name with stack\n+            role_arn=pipeline_role.arn,\n+            artifact_stores=[\n+                PipelineArtifactStoreArgs(\n+                    type=\"S3\",\n+                    location=build_bucket.bucket,\n+                )\n+            ],\n+            stages=[\n+                PipelineStageArgs(\n+                    name=\"Source\",\n+                    actions=[\n+                        PipelineStageActionArgs(\n+                            name=\"Source\",\n+                            category=\"Source\",\n+                            owner=\"AWS\",\n+                            provider=\"S3\",\n+                            version=\"1\",\n+                            output_artifacts=[\"SourceArtifact\"],\n+                            configuration={\n+                                \"S3Bucket\": build_bucket.bucket,\n+                                \"S3ObjectKey\": f\"{self.name}/source.zip\",\n+                            },\n+                            run_order=1,\n+                        )\n+                    ],\n+                ),\n+                PipelineStageArgs(\n+                    name=\"Build\",\n+                    actions=[\n+                        PipelineStageActionArgs(\n+                            name=f\"Build_py{v.replace('.', '')}\",\n+                            category=\"Build\",\n+                            owner=\"AWS\",\n+                            provider=\"CodeBuild\",\n+                            version=\"1\",\n+                            input_artifacts=[\"SourceArtifact\"],\n+                            output_artifacts=[f\"py{v.replace('.', '')}\"],\n+                            run_order=1,\n+                            configuration={\n+                                \"ProjectName\": build_projects[v].name,\n+                            },\n+                        )\n+                        for v in self.python_versions\n+                    ],\n+                ),\n+                PipelineStageArgs(\n+                    name=\"Deploy\",\n+                    actions=[\n+                        PipelineStageActionArgs(\n+                            name=\"PublishAndUpdate\",\n+                            category=\"Build\",\n+                            owner=\"AWS\",\n+                            provider=\"CodeBuild\",\n+                            version=\"1\",\n+                            input_artifacts=[\n+                                f\"py{v.replace('.', '')}\"\n+                                for v in self.python_versions\n+                            ],\n+                            run_order=1,\n+                            configuration={\n+                                \"ProjectName\": publish_project.name,\n+                                \"PrimarySource\": f\"py{self.python_versions[0].replace('.', '')}\",\n+                            },\n+                        )\n+                    ],\n+                ),\n+            ],\n+            opts=pulumi.ResourceOptions(\n+                parent=self,\n+                depends_on=(\n+                    [_bucket_versioning]\n+                    if _bucket_versioning is not None\n+                    else None\n+                ),\n+            ),\n+        )\n+\n+        # Trigger pipeline run when source is updated\n+        trigger_script = pipeline.name.apply(\n+            lambda pn: f\"\"\"#!/usr/bin/env bash\n+set -e\n+echo \"üîÑ Changes detected, starting CodePipeline execution for {self.name}\"\n+EXEC_ID=$(aws codepipeline start-pipeline-execution --name {pn} \\\n+  --query pipelineExecutionId --output text)\n+echo \"Triggered pipeline: $EXEC_ID\"\n+\"\"\"\n+        )\n+        # Only create trigger-pipeline command if NOT in sync mode (to avoid duplicate triggers)\n+        if not self.sync_mode:\n+            command.local.Command(\n+                f\"{self.name}-trigger-pipeline\",\n+                create=trigger_script,\n+                update=trigger_script,\n+                triggers=[package_hash],\n+                opts=pulumi.ResourceOptions(\n+                    parent=self,\n+                    depends_on=[upload_cmd, pipeline],\n+                ),\n+            )\n+\n+        # If sync_mode, start the pipeline and wait for it to complete before finishing Pulumi up\n+        if self.sync_mode:\n+            sync_script = pipeline.name.apply(\n+                lambda pn: f\"\"\"#!/usr/bin/env bash\n+set -e\n+echo \"üîÑ Sync: Starting CodePipeline execution for {self.name}\"\n+EXEC_ID=$(aws codepipeline start-pipeline-execution --name {pn} \\\n+  --query pipelineExecutionId --output text)\n+echo \"Execution ID: $EXEC_ID\"\n+sleep 2\n+while true; do\n+    STATUS=$(aws codepipeline get-pipeline-execution \\\\\n+        --pipeline-name {pn} \\\\\n+        --pipeline-execution-id $EXEC_ID \\\\\n+        --query \"pipelineExecution.status\" --output text)\n+    echo \"üîÑ Pipeline status: $STATUS\"\n+    if [ \"$STATUS\" = \"Succeeded\" ]; then\n+        echo \"‚úÖ Pipeline completed successfully\"\n+        break\n+    elif [ \"$STATUS\" = \"Failed\" ] || [ \"$STATUS\" = \"Superseded\" ]; then\n+        echo \"‚ùå Pipeline failed with status: $STATUS\"\n+        exit 1\n+    fi\n+    sleep 10\n+done\n+\"\"\"\n+            )\n+            sync_cmd = command.local.Command(\n+                f\"{self.name}-sync-pipeline\",\n+                create=sync_script,\n+                update=sync_script,  # Also run on updates\n+                triggers=[package_hash],  # Trigger on package changes\n+                opts=pulumi.ResourceOptions(\n+                    parent=self, depends_on=[upload_cmd, pipeline]\n+                ),\n+            )\n+            # Ensure Pulumi waits for pipeline before proceeding\n+            pulumi.log.info(f\"Sync command added for pipeline {self.name}\")\n+        else:\n+            sync_cmd = None\n+\n+        # In sync mode, create LayerVersion resource and wait for pipeline\n+        # In async mode, create LayerVersion resource for ARN but let pipeline manage updates\n+        self.layer_version = LayerVersion(\n+            f\"{self.name}-lambda-layer\",\n+            layer_name=self.layer_name,\n+            compatible_runtimes=[f\"python{v}\" for v in self.python_versions],\n+            compatible_architectures=[\"x86_64\", \"arm64\"],\n+            description=self.description,\n+            s3_bucket=build_bucket.bucket,\n+            s3_key=f\"{self.name}/combined/layer.zip\",\n+            opts=pulumi.ResourceOptions(\n+                depends_on=(\n+                    [sync_cmd] if (self.sync_mode and sync_cmd) else [pipeline]\n+                ),\n+                parent=self,\n+                # Let pipeline manage the actual layer content via aws lambda publish-layer-version\n+                ignore_changes=[\"s3_key\"] if not self.sync_mode else None,\n+            ),\n+        )\n+        self.arn = self.layer_version.arn\n+\n+    def _create_and_run_upload_script(\n+        self, bucket: str, package_path: str, package_hash: str\n+    ) -> str:\n+        \"\"\"Create a script file and return just the execution command.\"\"\"\n+        try:\n+            # Generate the script content with embedded variables to avoid argument issues\n+            script_content = self._generate_upload_script(\n+                bucket, package_path, package_hash\n+            )\n+\n+            # Create a persistent script file in /tmp with a unique name\n+            script_name = f\"pulumi-upload-{self.name}-{package_hash[:8]}.sh\"\n+            script_path = os.path.join(\"/tmp\", script_name)\n+\n+            # Write the script file\n+            with open(script_path, \"w\") as f:\n+                f.write(script_content)\n+\n+            # Make it executable\n+            os.chmod(script_path, 0o755)\n+\n+            # Return just the simple command to execute the script\n+            # No arguments or environment variables in the command line\n+            return f\"/bin/bash {script_path}\"\n+        except (OSError, IOError) as e:\n+            raise RuntimeError(f\"Failed to create upload script: {e}\") from e\n+\n+    def _generate_upload_script(\n+        self, bucket: str, package_path: str, package_hash: str\n+    ) -> str:\n+        \"\"\"Generate script to upload source package with safely embedded paths.\n+\n+        Includes improved error handling and validation.\n+        \"\"\"\n+        # Escape the paths to handle special characters\n+        safe_package_path = shlex.quote(package_path)\n+        safe_bucket = shlex.quote(bucket)\n+\n+        # Get local dependencies that need to be included\n+        local_deps = self._get_local_dependencies()\n+\n+        return f\"\"\"#!/bin/bash\n+set -e\n+\n+# Set variables within the script to avoid command line length issues\n+BUCKET={safe_bucket}\n+PACKAGE_PATH={safe_package_path}\n+HASH=\"{package_hash}\"\n+LAYER_NAME=\"{self.name}\"\n+FORCE_REBUILD=\"{self.force_rebuild}\"\n+LOCAL_DEPS=\"{' '.join(local_deps)}\"\n+\n+echo \"üì¶ Checking if source upload needed for layer '$LAYER_NAME'...\"\n+if [ -n \"$LOCAL_DEPS\" ]; then\n+    echo \"üìö Including local dependencies: $LOCAL_DEPS\"\n+fi\n+\n+# Check if we need to upload\n+STORED_HASH=$(aws s3 cp \"s3://$BUCKET/$LAYER_NAME/hash.txt\" - 2>/dev/null || echo '')\n+if [ \"$STORED_HASH\" = \"$HASH\" ] && [ \"$FORCE_REBUILD\" != \"True\" ]; then\n+    HASH_SHORT=$(echo \"$HASH\" | cut -c1-12)\n+    echo \"‚úÖ Source already up-to-date (hash: $HASH_SHORT...). Skipping upload.\"\n+    exit 0\n+fi\n+\n+if [ \"$STORED_HASH\" != \"$HASH\" ]; then\n+    echo \"üìù Source changes detected, uploading...\"\n+elif [ \"$FORCE_REBUILD\" = \"True\" ]; then\n+    echo \"üî® Force rebuild enabled, re-uploading source...\"\n+fi\n+\n+# Validate package structure before upload\n+if [ ! -f \"$PACKAGE_PATH/pyproject.toml\" ]; then\n+    echo \"‚ùå Error: pyproject.toml not found in $PACKAGE_PATH\"\n+    echo \"Package contents:\"\n+    ls -la \"$PACKAGE_PATH\" 2>/dev/null || echo \"Directory not accessible\"\n+    echo \"Current directory: $(pwd)\"\n+    echo \"Looking for package in: $PACKAGE_PATH\"\n+    exit 1\n+fi\n+\n+# Check if package has Python files\n+PY_FILES=$(find \"$PACKAGE_PATH\" -name \"*.py\" -type f | head -5)\n+if [ -z \"$PY_FILES\" ]; then\n+    echo \"‚ö†Ô∏è  Warning: No Python files found in package\"\n+else\n+    echo \"‚úì Found Python files in package\"\n+fi\n+\n+# Upload source\n+TMP_DIR=$(mktemp -d)\n+trap 'rm -rf \"$TMP_DIR\"' EXIT\n+\n+echo \"Creating source package structure...\"\n+mkdir -p \"$TMP_DIR/source\"\n+\n+# Copy the main package directory structure\n+cp -r \"$PACKAGE_PATH\"/* \"$TMP_DIR/source/\"\n+\n+# Include local dependencies if present\n+if [ -n \"$LOCAL_DEPS\" ]; then\n+    echo \"Including local dependencies in source package...\"\n+    for dep in $LOCAL_DEPS; do\n+        DEP_PATH=\"$(dirname \"$PACKAGE_PATH\")/$dep\"\n+        if [ -d \"$DEP_PATH\" ]; then\n+            echo \"  - Adding $dep from $DEP_PATH\"\n+            mkdir -p \"$TMP_DIR/dependencies/$dep\"\n+            cp -r \"$DEP_PATH\"/* \"$TMP_DIR/dependencies/$dep/\"\n+        else\n+            echo \"  Warning: Local dependency $dep not found at $DEP_PATH\"\n+        fi\n+    done\n+fi\n+\n+# Create zip quietly to reduce output\n+cd \"$TMP_DIR\"\n+if [ -d dependencies ]; then\n+    zip -qr source.zip source dependencies\n+else\n+    zip -qr source.zip source\n+fi\n+cd - >/dev/null\n+\n+echo \"Uploading to S3...\"\n+# Add retry logic for S3 operations\n+for attempt in 1 2 3; do\n+    if aws s3 cp \"$TMP_DIR/source.zip\" \"s3://$BUCKET/$LAYER_NAME/source.zip\"; then\n+        echo \"‚úì Source zip uploaded\"\n+        break\n+    else\n+        echo \"Attempt $attempt failed, retrying...\"\n+        sleep 2\n+    fi\n+    if [ $attempt -eq 3 ]; then\n+        echo \"‚ùå Failed to upload source.zip after 3 attempts\"\n+        exit 1\n+    fi\n+done\n+\n+echo -n \"$HASH\" | aws s3 cp - \"s3://$BUCKET/$LAYER_NAME/hash.txt\"\n+\n+echo \"‚úÖ Source uploaded successfully\"\n+\"\"\"\n+\n+    def _generate_trigger_script(\n+        self, bucket: str, project_name: str, package_hash: str\n+    ) -> str:\n+        \"\"\"Generate script to trigger build without waiting.\"\"\"\n+        return f\"\"\"#!/bin/bash\n+set -e\n+\n+BUCKET=\"{bucket}\"\n+PROJECT=\"{project_name}\"\n+HASH=\"{package_hash}\"\n+\n+echo \"üöÄ Checking if build needed for layer '{self.name}'...\"\n+\n+# Check if we need to build\n+STORED_HASH=$(aws s3 cp s3://$BUCKET/{self.name}/hash.txt - 2>/dev/null || echo '')\n+if [ \"$STORED_HASH\" = \"$HASH\" ] && [ \"{self.force_rebuild}\" != \"True\" ]; then\n+    HASH_SHORT=$(echo \"$HASH\" | cut -c1-12)\n+    echo \"‚úÖ No changes detected (hash: $HASH_SHORT...). Skipping build.\"\n+    echo \"üí° To force rebuild: pulumi up --config lambda-layer:force-rebuild=true\"\n+    exit 0\n+fi\n+\n+if [ \"$STORED_HASH\" != \"$HASH\" ]; then\n+    echo \"üìù Code changes detected:\"\n+    STORED_HASH_SHORT=$(echo \"$STORED_HASH\" | cut -c1-12)\n+    HASH_SHORT=$(echo \"$HASH\" | cut -c1-12)\n+    echo \"   Old hash: $STORED_HASH_SHORT...\"\n+    echo \"   New hash: $HASH_SHORT...\"\n+fi\n+\n+if [ \"{self.force_rebuild}\" = \"True\" ]; then\n+    echo \"üî® Force rebuild enabled\"\n+fi\n+\n+# Check if there's already a build in progress\n+BUILD_STATUS=$(aws codebuild list-builds-for-project --project-name \"$PROJECT\" \\\n+  --query 'ids[0]' --output text 2>/dev/null || echo \"None\")\n+if [ \"$BUILD_STATUS\" != \"None\" ]; then\n+    CURRENT_STATUS=$(aws codebuild batch-get-builds --ids \"$BUILD_STATUS\" \\\n+      --query 'builds[0].buildStatus' --output text 2>/dev/null || echo \"UNKNOWN\")\n+    if [ \"$CURRENT_STATUS\" = \"IN_PROGRESS\" ]; then\n+        echo \"‚è≥ Build already in progress. Skipping new build.\"\n+        exit 0\n+    fi\n+fi\n+\n+# Start async build\n+echo \"üèóÔ∏è  Starting async build...\"\n+BUILD_ID=$(aws codebuild start-build --project-name \"$PROJECT\" --query 'build.id' --output text)\n+echo \"‚úÖ Build started: $BUILD_ID\"\n+echo \"üìä Monitor at: https://console.aws.amazon.com/codesuite/codebuild/projects/$PROJECT/\\\\\"\n+echo \"build/$BUILD_ID\"\n+echo \"‚ö° Continuing with fast pulumi up (not waiting for completion)\"\n+\"\"\"\n+\n+    def _generate_initial_build_script(\n+        self, bucket: str, project_name: str\n+    ) -> str:\n+        \"\"\"Generate script to ensure initial layer exists.\"\"\"\n+        return f\"\"\"#!/bin/bash\n+set -e\n+\n+BUCKET=\"{bucket}\"\n+PROJECT=\"{project_name}\"\n+\n+echo \"üîç Checking if initial layer exists for '{self.name}'...\"\n+\n+# Check if layer exists\n+if aws s3api head-object --bucket \"$BUCKET\" --key \"{self.name}/layer.zip\" &>/dev/null; then\n+    echo \"‚úÖ Layer already exists. No initial build needed.\"\n+    exit 0\n+fi\n+\n+echo \"üèóÔ∏è  No layer found. Running initial build (this will wait for completion)...\"\n+\n+# Start build and wait\n+BUILD_ID=$(aws codebuild start-build --project-name \"$PROJECT\" --query 'build.id' --output text)\n+echo \"Build ID: $BUILD_ID\"\n+\n+while true; do\n+BUILD_STATUS=$(aws codebuild batch-get-builds --ids \"$BUILD_ID\" \\\n+  --query 'builds[0].buildStatus' --output text)\n+    echo \"Build status: $BUILD_STATUS\"\n+\n+    if [ \"$BUILD_STATUS\" = \"SUCCEEDED\" ]; then\n+        echo \"‚úÖ Initial build completed successfully!\"\n+        break\n+elif [ \"$BUILD_STATUS\" = \"FAILED\" ] || [ \"$BUILD_STATUS\" = \"FAULT\" ] || \\\n+     [ \"$BUILD_STATUS\" = \"STOPPED\" ] || [ \"$BUILD_STATUS\" = \"TIMED_OUT\" ]; then\n+        echo \"‚ùå Initial build failed with status: $BUILD_STATUS\"\n+        exit 1\n+    fi\n+\n+    sleep 30\n+done\n+\"\"\"\n+\n+    def _generate_sync_script(\n+        self,\n+        bucket: str,\n+        project_name: str,\n+        layer_name: str,\n+        package_path: str,\n+        package_hash: str,\n+    ) -> str:\n+        \"\"\"Generate script for sync mode (waits for completion).\"\"\"\n+        return f\"\"\"#!/bin/bash\n+set -e\n+\n+BUCKET=\"{bucket}\"\n+PROJECT=\"{project_name}\"\n+LAYER_NAME=\"{layer_name}\"\n+PACKAGE_PATH=\"{package_path}\"\n+HASH=\"{package_hash}\"\n+\n+echo \"üîÑ Building layer '{self.name}' in SYNC mode...\"\n+\n+# Check if we need to rebuild\n+if [ \"$(aws s3 cp s3://$BUCKET/{self.name}/hash.txt - 2>/dev/null || echo '')\" \\\n+     = \"$HASH\" ] && [ \"{self.force_rebuild}\" != \"True\" ]; then\n+    echo \"‚úÖ No changes detected. Skipping rebuild.\"\n+\n+    # Ensure layer exists\n+    if ! aws s3api head-object --bucket \"$BUCKET\" --key \"{self.name}/layer.zip\" &>/dev/null; then\n+        echo \"‚ùå Layer missing but hash matches. Please run with force-rebuild.\"\n+        exit 1\n+    fi\n+    exit 0\n+fi\n+\n+# Upload source\n+echo \"üì¶ Uploading source...\"\n+TMP_DIR=$(mktemp -d)\n+trap 'rm -rf \"$TMP_DIR\"' EXIT\n+\n+mkdir -p \"$TMP_DIR/source\"\n+cp -r \"$PACKAGE_PATH\"/* \"$TMP_DIR/source/\"\n+\n+# Use cd instead of pushd/popd for better shell compatibility\n+cd \"$TMP_DIR\"\n+zip -r source.zip source\n+cd - >/dev/null\n+\n+aws s3 cp \"$TMP_DIR/source.zip\" \"s3://$BUCKET/{self.name}/source.zip\"\n+\n+# Start build and wait\n+echo \"üèóÔ∏è  Starting build...\"\n+BUILD_ID=$(aws codebuild start-build --project-name \"$PROJECT\" --query 'build.id' --output text)\n+echo \"Build ID: $BUILD_ID\"\n+\n+while true; do\n+BUILD_STATUS=$(aws codebuild batch-get-builds --ids \"$BUILD_ID\" \\\n+  --query 'builds[0].buildStatus' --output text)\n+    echo \"Build status: $BUILD_STATUS\"\n+\n+    if [ \"$BUILD_STATUS\" = \"SUCCEEDED\" ]; then\n+        echo \"‚úÖ Build completed successfully!\"\n+        break\n+elif [ \"$BUILD_STATUS\" = \"FAILED\" ] || [ \"$BUILD_STATUS\" = \"FAULT\" ] || \\\n+     [ \"$BUILD_STATUS\" = \"STOPPED\" ] || [ \"$BUILD_STATUS\" = \"TIMED_OUT\" ]; then\n+        echo \"‚ùå Build failed with status: $BUILD_STATUS\"\n+        exit 1\n+    fi\n+\n+    sleep 30\n+done\n+\n+# Save hash\n+echo -n \"$HASH\" | aws s3 cp - \"s3://$BUCKET/{self.name}/hash.txt\"\n+echo \"‚úÖ Layer build process completed!\"\n+\"\"\"\n+\n+\n+# Define the layers to build\n+layers_to_build = [\n+    {\n+        \"package_dir\": \"receipt_dynamo\",\n+        \"name\": \"receipt-dynamo\",\n+        \"description\": \"DynamoDB layer for receipt-dynamo\",\n+        \"python_versions\": [\"3.12\"],\n+        \"needs_pillow\": False,\n+    },\n+    {\n+        \"package_dir\": \"receipt_label\",\n+        \"name\": \"receipt-label\",\n+        \"description\": \"Label layer for receipt-label\",\n+        \"python_versions\": [\"3.12\"],\n+        \"needs_pillow\": True,  # Needed - transitive dependency via openai or other packages\n+        \"package_extras\": \"lambda\",  # Minimal dependencies for Lambda\n+    },\n+    {\n+        \"package_dir\": \"receipt_upload\",\n+        \"name\": \"receipt-upload\",\n+        \"description\": \"Upload layer for receipt-upload\",\n+        \"python_versions\": [\"3.12\"],\n+        \"needs_pillow\": False,  # Not needed - no image processing in upload lambdas\n+    },\n+]\n+\n+# Create Lambda layers using the fast approach\n+# TEMPORARILY SKIP LAYER BUILDING\n+SKIP_LAYER_BUILDING = (\n+    os.environ.get(\"PYTEST_RUNNING\") == \"1\" or False\n+)  # Skip building during tests\n+SKIP_LAYER_BUILDING = (\n+    os.environ.get(\"PYTEST_RUNNING\") == \"1\" or False\n+)  # Skip building during tests","path":"infra/components/lambda_layer.py","commit_id":"b4b5b64f14380322d789450da96a71cea99e1006","original_commit_id":"a305f6b5943e8ce8e40ab9a9d7ca3315da013746","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üü° Minor_\n\n**Remove duplicate assignment.**\n\n`SKIP_LAYER_BUILDING` is assigned twice with identical values.\n\n```diff\n # Create Lambda layers using the fast approach\n # TEMPORARILY SKIP LAYER BUILDING\n SKIP_LAYER_BUILDING = (\n     os.environ.get(\"PYTEST_RUNNING\") == \"1\" or False\n )  # Skip building during tests\n-SKIP_LAYER_BUILDING = (\n-    os.environ.get(\"PYTEST_RUNNING\") == \"1\" or False\n-)  # Skip building during tests\n```\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn infra/components/lambda_layer.py around lines 1431 to 1436 there is a\nduplicate assignment of SKIP_LAYER_BUILDING with the same expression; remove the\nredundant line so SKIP_LAYER_BUILDING is assigned only once, leaving the single\nintended assignment that reads SKIP_LAYER_BUILDING =\n(os.environ.get(\"PYTEST_RUNNING\") == \"1\" or False) and keep the trailing comment\nif desired.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:ocelot -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commit 6ae1f43","created_at":"2025-12-06T03:01:34Z","updated_at":"2025-12-06T03:21:02Z","html_url":"https://github.com/tnorlund/Portfolio/pull/491#discussion_r2594458653","pull_request_url":"https://api.github.com/repos/tnorlund/Portfolio/pulls/491","_links":{"self":{"href":"https://api.github.com/repos/tnorlund/Portfolio/pulls/comments/2594458653"},"html":{"href":"https://github.com/tnorlund/Portfolio/pull/491#discussion_r2594458653"},"pull_request":{"href":"https://api.github.com/repos/tnorlund/Portfolio/pulls/491"}},"reactions":{"url":"https://api.github.com/repos/tnorlund/Portfolio/pulls/comments/2594458653/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":1405,"original_start_line":1431,"start_side":"RIGHT","line":1407,"original_line":1436,"side":"RIGHT","author_association":"NONE","original_position":1436,"position":1407,"subject_type":"line"},{"url":"https://api.github.com/repos/tnorlund/Portfolio/pulls/comments/2594458654","pull_request_review_id":3547057551,"id":2594458654,"node_id":"PRRC_kwDOM8BQvs6apEwe","diff_hunk":"@@ -1,3 +1,21 @@\n+\"\"\"\n+Compatibility shim for legacy imports.\n+\"\"\"\n+\n+from infra.components.ecs_lambda import *  # noqa: F401,F403\n+\n+\"\"\"\n+Compatibility shim for legacy imports.\n+\"\"\"\n+\n+from infra.components.ecs_lambda import *  # noqa: F401,F403\n+\n+\"\"\"\n+Compatibility shim for legacy imports.\n+\"\"\"\n+\n+from infra.components.ecs_lambda import *  # noqa: F401,F403\n+","path":"infra/ecs_lambda.py","commit_id":"a305f6b5943e8ce8e40ab9a9d7ca3315da013746","original_commit_id":"a305f6b5943e8ce8e40ab9a9d7ca3315da013746","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Critical: Remove duplicate compatibility shim code.**\n\nThe compatibility shim is duplicated three times. This will cause `SyntaxError` or unexpected behavior. The file should contain only one shim block if it's meant to be a compatibility shim, or the actual implementation if it's meant to be the component module.\n\nBased on the file path (`infra/ecs_lambda.py`) and the AI summary indicating this should be a \"compatibility shim that re-exports from the new location,\" this file should contain only:\n\n```diff\n-\"\"\"\n-Compatibility shim for legacy imports.\n-\"\"\"\n-\n-from infra.components.ecs_lambda import *  # noqa: F401,F403\n-\n-\"\"\"\n-Compatibility shim for legacy imports.\n-\"\"\"\n-\n-from infra.components.ecs_lambda import *  # noqa: F401,F403\n-\n \"\"\"\n Compatibility shim for legacy imports.\n \"\"\"\n\n from infra.components.ecs_lambda import *  # noqa: F401,F403\n-\n-#!/usr/bin/env python3\n-...rest of file should be removed if this is a shim...\n```\n\nIf this file is meant to contain the actual `EcsLambda` implementation (and `infra/components/ecs_lambda.py` re-exports from here), then remove all shim blocks and keep only the implementation starting at line 19.\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n\"\"\"\nCompatibility shim for legacy imports.\n\"\"\"\n\nfrom infra.components.ecs_lambda import *  # noqa: F401,F403\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>üß∞ Tools</summary>\n\n<details>\n<summary>ü™õ Ruff (0.14.7)</summary>\n\n5-5: Unused `noqa` directive (non-enabled: `F401`)\n\nRemove unused `noqa` directive\n\n(RUF100)\n\n---\n\n11-11: Unused `noqa` directive (non-enabled: `F401`)\n\nRemove unused `noqa` directive\n\n(RUF100)\n\n---\n\n17-17: Unused `noqa` directive (non-enabled: `F401`)\n\nRemove unused `noqa` directive\n\n(RUF100)\n\n</details>\n\n</details>\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\ninfra/ecs_lambda.py lines 1-18: the compatibility shim (docstring + \"from\ninfra.components.ecs_lambda import *  # noqa: F401,F403\") is duplicated three\ntimes; remove the duplicate blocks so the file contains either a single shim\n(one docstring and one import) or, if this file should host the real\nimplementation, delete all shim blocks and retain only the actual implementation\nstarting at line 19.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:ocelot -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commit 6ae1f43","created_at":"2025-12-06T03:01:34Z","updated_at":"2025-12-06T03:21:07Z","html_url":"https://github.com/tnorlund/Portfolio/pull/491#discussion_r2594458654","pull_request_url":"https://api.github.com/repos/tnorlund/Portfolio/pulls/491","_links":{"self":{"href":"https://api.github.com/repos/tnorlund/Portfolio/pulls/comments/2594458654"},"html":{"href":"https://github.com/tnorlund/Portfolio/pull/491#discussion_r2594458654"},"pull_request":{"href":"https://api.github.com/repos/tnorlund/Portfolio/pulls/491"}},"reactions":{"url":"https://api.github.com/repos/tnorlund/Portfolio/pulls/comments/2594458654/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":1,"start_side":"RIGHT","line":null,"original_line":18,"side":"RIGHT","author_association":"NONE","original_position":18,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/tnorlund/Portfolio/pulls/comments/2594458658","pull_request_review_id":3547057551,"id":2594458658,"node_id":"PRRC_kwDOM8BQvs6apEwi","diff_hunk":"@@ -14,21 +32,46 @@\n Usage pattern mirrors `LambdaLayer` but targets function packaging and updates.\n \"\"\"\n \n+# pylint: disable=import-error\n+\n import base64\n import glob\n-import hashlib\n import json\n import os\n import shlex\n from typing import Any, Dict, List, Optional\n \n import pulumi\n-import pulumi_aws as aws\n import pulumi_command as command\n-from pulumi import AssetArchive, ComponentResource, Output, ResourceOptions, StringAsset\n+from build_utils import compute_hash, make_artifact_bucket, make_log_group","path":"infra/ecs_lambda.py","commit_id":"a305f6b5943e8ce8e40ab9a9d7ca3315da013746","original_commit_id":"a305f6b5943e8ce8e40ab9a9d7ca3315da013746","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Critical: Fix import path for `build_utils`.**\n\nThis import will fail at runtime. The module is located at `infra.shared.build_utils`, not `build_utils`. Compare with `infra/components/lambda_layer.py` which uses the correct path.\n\n```diff\n-from build_utils import compute_hash, make_artifact_bucket, make_log_group\n+from infra.shared.build_utils import compute_hash, make_artifact_bucket, make_log_group\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\nfrom infra.shared.build_utils import compute_hash, make_artifact_bucket, make_log_group\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn infra/ecs_lambda.py around line 46 the import \"from build_utils import\ncompute_hash, make_artifact_bucket, make_log_group\" uses the wrong module path\nand will fail at runtime; update the import to reference the correct package\npath \"infra.shared.build_utils\" (e.g., from infra.shared.build_utils import\ncompute_hash, make_artifact_bucket, make_log_group) and ensure any relative\nreferences to these symbols in the file remain unchanged.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:ocelot -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-12-06T03:01:34Z","updated_at":"2025-12-06T03:01:36Z","html_url":"https://github.com/tnorlund/Portfolio/pull/491#discussion_r2594458658","pull_request_url":"https://api.github.com/repos/tnorlund/Portfolio/pulls/491","_links":{"self":{"href":"https://api.github.com/repos/tnorlund/Portfolio/pulls/comments/2594458658"},"html":{"href":"https://github.com/tnorlund/Portfolio/pull/491#discussion_r2594458658"},"pull_request":{"href":"https://api.github.com/repos/tnorlund/Portfolio/pulls/491"}},"reactions":{"url":"https://api.github.com/repos/tnorlund/Portfolio/pulls/comments/2594458658/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":null,"original_line":46,"side":"RIGHT","author_association":"NONE","original_position":40,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/tnorlund/Portfolio/pulls/comments/2594458660","pull_request_review_id":3547057551,"id":2594458660,"node_id":"PRRC_kwDOM8BQvs6apEwk","diff_hunk":"@@ -14,21 +32,46 @@\n Usage pattern mirrors `LambdaLayer` but targets function packaging and updates.\n \"\"\"\n \n+# pylint: disable=import-error\n+\n import base64\n import glob\n-import hashlib\n import json\n import os\n import shlex\n from typing import Any, Dict, List, Optional\n \n import pulumi\n-import pulumi_aws as aws\n import pulumi_command as command\n-from pulumi import AssetArchive, ComponentResource, Output, ResourceOptions, StringAsset\n+from build_utils import compute_hash, make_artifact_bucket, make_log_group\n+from pulumi import (\n+    AssetArchive,\n+    ComponentResource,\n+    Output,\n+    ResourceOptions,\n+    StringAsset,\n+)\n+from pulumi_aws import config, get_caller_identity\n+from pulumi_aws.codebuild import (\n+    Project,\n+    ProjectArtifactsArgs,\n+    ProjectEnvironmentArgs,\n+    ProjectEnvironmentEnvironmentVariableArgs,\n+    ProjectLogsConfigArgs,\n+    ProjectLogsConfigCloudwatchLogsArgs,\n+    ProjectSourceArgs,\n+)\n+from pulumi_aws.codepipeline import (\n+    Pipeline,\n+    PipelineArtifactStoreArgs,\n+    PipelineStageActionArgs,\n+    PipelineStageArgs,\n+)\n+from pulumi_aws.iam import Role as ROLE\n+from pulumi_aws.iam import RolePolicy, RolePolicyAttachment\n+from pulumi_aws.lambda_ import Function, FunctionEnvironmentArgs\n from utils import _find_project_root","path":"infra/ecs_lambda.py","commit_id":"a305f6b5943e8ce8e40ab9a9d7ca3315da013746","original_commit_id":"a305f6b5943e8ce8e40ab9a9d7ca3315da013746","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Critical: Fix import path for `utils`.**\n\nThis import will fail at runtime. The module is located at `infra.utils`, not `utils`.\n\n```diff\n-from utils import _find_project_root\n+from infra.utils import _find_project_root\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\nfrom infra.utils import _find_project_root\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn infra/ecs_lambda.py at line 73, the import currently references a top-level\nutils module which will fail at runtime; change the import to reference the\npackage-local module by importing from infra.utils (i.e., replace the existing\nimport with one that imports _find_project_root from infra.utils) and run a\nquick import/test to confirm resolution.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:ocelot -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-12-06T03:01:34Z","updated_at":"2025-12-06T03:01:36Z","html_url":"https://github.com/tnorlund/Portfolio/pull/491#discussion_r2594458660","pull_request_url":"https://api.github.com/repos/tnorlund/Portfolio/pulls/491","_links":{"self":{"href":"https://api.github.com/repos/tnorlund/Portfolio/pulls/comments/2594458660"},"html":{"href":"https://github.com/tnorlund/Portfolio/pull/491#discussion_r2594458660"},"pull_request":{"href":"https://api.github.com/repos/tnorlund/Portfolio/pulls/491"}},"reactions":{"url":"https://api.github.com/repos/tnorlund/Portfolio/pulls/comments/2594458660/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":null,"original_line":73,"side":"RIGHT","author_association":"NONE","original_position":67,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/tnorlund/Portfolio/pulls/comments/2594486446","pull_request_review_id":3547107753,"id":2594486446,"node_id":"PRRC_kwDOM8BQvs6apLiu","diff_hunk":"@@ -0,0 +1,1007 @@\n+#!/usr/bin/env python3\n+\"\"\"\n+codebuild_docker_image.py\n+\n+AWS CodeBuild-based Docker image builder for Lambda functions.\n+Offloads Docker builds to AWS for faster pulumi up times.\n+\n+Similar to EcsLambda but for Docker images:\n+- Fast `pulumi up` (async by default)\n+- Simple architecture: S3 ‚Üí CodePipeline ‚Üí CodeBuild ‚Üí ECR ‚Üí Lambda\n+- Maintains Pulumi state\n+- Supports multi-stage builds with layer caching\n+\"\"\"\n+\n+# pylint: disable=import-error\n+\n+import json\n+import shlex\n+from pathlib import Path\n+from typing import Any, Dict, Optional\n+\n+import pulumi\n+import pulumi_command as command\n+from pulumi import ComponentResource, Output, ResourceOptions\n+from pulumi_aws import config, get_caller_identity\n+from pulumi_aws.codebuild import (\n+    Project,\n+    ProjectArtifactsArgs,\n+    ProjectCacheArgs,\n+    ProjectEnvironmentArgs,\n+    ProjectEnvironmentEnvironmentVariableArgs,\n+    ProjectLogsConfigArgs,\n+    ProjectLogsConfigCloudwatchLogsArgs,\n+    ProjectSourceArgs,\n+)\n+from pulumi_aws.codepipeline import (\n+    Pipeline,\n+    PipelineArtifactStoreArgs,\n+    PipelineStageActionArgs,\n+    PipelineStageArgs,\n+)\n+from pulumi_aws.ecr import Repository, RepositoryImageScanningConfigurationArgs\n+from pulumi_aws.iam import Role as ROLE\n+from pulumi_aws.iam import RolePolicy\n+from pulumi_aws.lambda_ import (\n+    Function,\n+    FunctionEnvironmentArgs,\n+    FunctionEphemeralStorageArgs,\n+    FunctionFileSystemConfigArgs,\n+    FunctionImageConfigArgs,\n+    FunctionVpcConfigArgs,\n+)\n+\n+from infra.shared.build_utils import (\n+    compute_hash,\n+    make_artifact_bucket,\n+    make_log_group,\n+    resolve_build_config,\n+)\n+from infra.shared.buildspecs import docker_image_buildspec\n+from infra.utils import _find_project_root\n+\n+PROJECT_DIR = _find_project_root()\n+\n+\n+class CodeBuildDockerImage(ComponentResource):\n+    \"\"\"AWS CodeBuild-based Docker image builder with ECR push and Lambda update.\n+\n+    This component:\n+    - Creates ECR repository\n+    - Uploads Dockerfile + build context to S3\n+    - Runs CodeBuild to build multi-stage Docker image\n+    - Pushes to ECR with content-based tags\n+    - Updates Lambda function with new image URI\n+    - Fast pulumi up (async by default, sync in CI)\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        name: str,\n+        *,\n+        dockerfile_path: str,  # Path to Dockerfile relative to project root\n+        build_context_path: str,  # Path to build context (usually project root)\n+        source_paths: Optional[\n+            list[str]\n+        ] = None,  # Specific paths to include in build\n+        lambda_function_name: Optional[\n+            str\n+        ] = None,  # If provided, updates Lambda\n+        lambda_config: Optional[Dict[str, Any]] = None,  # Lambda configuration\n+        build_args: Optional[Dict[str, str]] = None,\n+        platform: str = \"linux/arm64\",\n+        sync_mode: Optional[bool] = None,\n+        lambda_aliases: Optional[\n+            list[str]\n+        ] = None,  # Pulumi aliases for Lambda rename\n+        opts: Optional[ResourceOptions] = None,\n+    ) -> None:\n+        super().__init__(f\"codebuild-docker:{name}\", name, {}, opts)\n+\n+        self.name = name\n+        # Keep paths relative - they'll be resolved from workspace root\n+        self.dockerfile_path = dockerfile_path\n+        self.build_context_path = build_context_path\n+        self.source_paths = (\n+            source_paths or []\n+        )  # Specific source paths for selective copying\n+        self.lambda_function_name = (\n+            lambda_function_name or f\"{name}-{pulumi.get_stack()}\"\n+        )\n+        self.lambda_config = lambda_config or {}\n+        self.build_args = build_args or {}\n+        self.platform = platform\n+        self.lambda_aliases = (\n+            lambda_aliases or []\n+        )  # Pulumi aliases for Lambda rename\n+\n+        # Configure build mode and flags\n+        (\n+            self.sync_mode,\n+            self.force_rebuild,\n+            self.debug_mode,\n+        ) = resolve_build_config(\n+            \"docker-build\",\n+            sync_override=sync_mode,\n+            ci_default_sync=True,\n+        )\n+\n+        # Calculate content hash for change detection\n+        content_hash = self._calculate_content_hash()\n+\n+        if self.sync_mode:\n+            pulumi.log.info(\n+                f\"üîÑ Building image '{self.name}' in SYNC mode (will wait)\"\n+            )\n+        else:\n+            pulumi.log.info(\n+                f\"‚ö° Image '{self.name}' in ASYNC mode (fast pulumi up)\"\n+            )\n+            pulumi.log.info(\n+                f\"   üì¶ Hash: {content_hash[:12]}... - will build only if changed\"\n+            )\n+\n+        # Create ECR repository\n+        self.ecr_repo = Repository(\n+            f\"{self.name}-repo\",\n+            image_scanning_configuration=RepositoryImageScanningConfigurationArgs(\n+                scan_on_push=True\n+            ),\n+            force_delete=True,\n+            opts=ResourceOptions(parent=self),\n+        )\n+\n+        # Setup build pipeline\n+        (\n+            _build_bucket,\n+            _upload_cmd,\n+            pipeline,\n+            _codebuild_project,\n+            pipeline_trigger_cmd,\n+        ) = self._setup_pipeline(content_hash)\n+\n+        # Push bootstrap image and create Lambda function if config provided\n+        if self.lambda_config:\n+            bootstrap_cmd = self._push_bootstrap_image()\n+            self._create_lambda_function(bootstrap_cmd, pipeline_trigger_cmd)\n+        else:\n+            self.lambda_function = None\n+\n+        # Export outputs\n+        self.repository_url = self.ecr_repo.repository_url\n+        self.image_uri = self.ecr_repo.repository_url.apply(\n+            lambda url: f\"{url}:latest\"\n+        )\n+        # Digest is managed by CodeBuild, provide a placeholder\n+        # The actual digest is used during Lambda updates in post_build phase\n+        self.digest = pulumi.Output.from_input(\"sha256:placeholder\")\n+\n+        # Self-reference for compatibility with components that wrap this\n+        self.docker_image = self\n+\n+        self.register_outputs(\n+            {\n+                \"repository_url\": self.repository_url,\n+                \"image_uri\": self.image_uri,\n+                \"digest\": self.digest,\n+            }\n+        )\n+\n+    def _calculate_content_hash(self) -> str:\n+        \"\"\"Calculate hash of Dockerfile and relevant context files.\"\"\"\n+        paths: list[Path] = []\n+\n+        # Include Dockerfile\n+        dockerfile = Path(PROJECT_DIR) / self.dockerfile_path\n+        if dockerfile.exists():\n+            paths.append(dockerfile)\n+\n+        # If source_paths specified, hash only those paths\n+        if self.source_paths:\n+            for source_path in sorted(self.source_paths):\n+                full_path = Path(PROJECT_DIR) / source_path\n+                if full_path.exists():\n+                    paths.append(full_path)\n+\n+            # ALWAYS hash the handler directory (Lambda-specific code)\n+            handler_dir = Path(PROJECT_DIR) / Path(self.dockerfile_path).parent\n+            if handler_dir.exists():\n+                for file_path in sorted(handler_dir.rglob(\"*.py\")):\n+                    if file_path.is_file() and not any(\n+                        part.startswith(\".\") or part == \"__pycache__\"\n+                        for part in file_path.parts\n+                    ):\n+                        paths.append(file_path)\n+        else:\n+            # Hash only the files that will be included in the build context\n+            if self.build_context_path == \".\":\n+                # Lambda images - hash Python packages and handler directory\n+                # Default packages that all Lambda images need\n+                packages_to_hash = [\n+                    \"receipt_dynamo/receipt_dynamo\",\n+                    \"receipt_dynamo/pyproject.toml\",\n+                    \"receipt_chroma/receipt_chroma\",\n+                    \"receipt_chroma/pyproject.toml\",\n+                    \"receipt_label/receipt_label\",\n+                    \"receipt_label/pyproject.toml\",\n+                ]\n+\n+                for package_path in packages_to_hash:\n+                    full_path = Path(PROJECT_DIR) / package_path\n+                    if full_path.exists():\n+                        paths.append(full_path)\n+\n+                # Also hash the handler directory\n+                handler_dir = (\n+                    Path(PROJECT_DIR) / Path(self.dockerfile_path).parent\n+                )\n+                if handler_dir.exists():\n+                    for file_path in sorted(handler_dir.rglob(\"*.py\")):\n+                        if file_path.is_file() and not any(\n+                            part.startswith(\".\") or part == \"__pycache__\"\n+                            for part in file_path.parts\n+                        ):\n+                            paths.append(file_path)\n+            else:\n+                # ECS images - hash the specific context directory\n+                context_path = Path(PROJECT_DIR) / self.build_context_path\n+                if context_path.exists():\n+                    paths.append(context_path)\n+\n+        return compute_hash(\n+            paths,\n+            include_globs=[\n+                \"**/*.py\",\n+                \"**/pyproject.toml\",\n+                \"**/requirements.txt\",\n+                \"Dockerfile\",\n+            ],\n+        )","path":"infra/components/codebuild_docker_image.py","commit_id":"b4b5b64f14380322d789450da96a71cea99e1006","original_commit_id":"b4b5b64f14380322d789450da96a71cea99e1006","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**Content hash misses default Lambda packages when `source_paths` is set.**\n\nWhen `self.source_paths` is non-empty and `build_context_path == \".\"`, `_calculate_content_hash` only hashes `source_paths` and the handler dir, but `_generate_upload_script` always rsyncs the default monorepo packages (`receipt_dynamo`, `receipt_chroma`, `receipt_label`) into the context for `CONTEXT_PATH=\".\"`.\n\nThis means changes in those default packages will not change `content_hash`, so:\n- The S3 `hash.txt` check can incorrectly consider the context up to date.\n- The `upload_cmd` / pipeline triggers (keyed by `content_hash`) will not fire, leaving images stale.\n\nYou should include the default packages in the hash even when `source_paths` is used. A minimal change inside `_calculate_content_hash` could look like:\n\n```diff\n-        # If source_paths specified, hash only those paths\n-        if self.source_paths:\n-            for source_path in sorted(self.source_paths):\n-                full_path = Path(PROJECT_DIR) / source_path\n-                if full_path.exists():\n-                    paths.append(full_path)\n-\n-            # ALWAYS hash the handler directory (Lambda-specific code)\n+        # If source_paths specified, hash those plus any shared packages\n+        if self.source_paths:\n+            for source_path in sorted(self.source_paths):\n+                full_path = Path(PROJECT_DIR) / source_path\n+                if full_path.exists():\n+                    paths.append(full_path)\n+\n+            # When building from the monorepo root, also hash the shared Lambda packages\n+            if self.build_context_path == \".\":\n+                packages_to_hash = [\n+                    \"receipt_dynamo/receipt_dynamo\",\n+                    \"receipt_dynamo/pyproject.toml\",\n+                    \"receipt_chroma/receipt_chroma\",\n+                    \"receipt_chroma/pyproject.toml\",\n+                    \"receipt_label/receipt_label\",\n+                    \"receipt_label/pyproject.toml\",\n+                ]\n+                for package_path in packages_to_hash:\n+                    full_path = Path(PROJECT_DIR) / package_path\n+                    if full_path.exists():\n+                        paths.append(full_path)\n+\n+            # ALWAYS hash the handler directory (Lambda-specific code)\n             handler_dir = Path(PROJECT_DIR) / Path(self.dockerfile_path).parent\n```\n\nThis keeps behavior for ECS images intact while making hash coverage match what actually gets copied into the Lambda context.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn infra/components/codebuild_docker_image.py around lines 190 to 259, when\nself.source_paths is non-empty the content-hash path list currently omits the\ndefault monorepo Lambda packages (receipt_dynamo, receipt_chroma, receipt_label)\neven though _generate_upload_script always rsyncs them when build_context_path\n== \".\", causing stale hashes; modify the branch under \"if self.source_paths:\"\nto, when self.build_context_path == \".\", also append the same packages_to_hash\n(check existence before appending) to paths in addition to the existing\nsource_paths and handler_dir logic so the computed hash includes those default\npackages; leave ECS/context-path behavior unchanged.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:olive -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-12-06T03:48:05Z","updated_at":"2025-12-06T03:48:06Z","html_url":"https://github.com/tnorlund/Portfolio/pull/491#discussion_r2594486446","pull_request_url":"https://api.github.com/repos/tnorlund/Portfolio/pulls/491","_links":{"self":{"href":"https://api.github.com/repos/tnorlund/Portfolio/pulls/comments/2594486446"},"html":{"href":"https://github.com/tnorlund/Portfolio/pull/491#discussion_r2594486446"},"pull_request":{"href":"https://api.github.com/repos/tnorlund/Portfolio/pulls/491"}},"reactions":{"url":"https://api.github.com/repos/tnorlund/Portfolio/pulls/comments/2594486446/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":190,"original_start_line":190,"start_side":"RIGHT","line":259,"original_line":259,"side":"RIGHT","author_association":"NONE","original_position":259,"position":259,"subject_type":"line"},{"url":"https://api.github.com/repos/tnorlund/Portfolio/pulls/comments/2594486448","pull_request_review_id":3547107753,"id":2594486448,"node_id":"PRRC_kwDOM8BQvs6apLiw","diff_hunk":"@@ -0,0 +1,1007 @@\n+#!/usr/bin/env python3\n+\"\"\"\n+codebuild_docker_image.py\n+\n+AWS CodeBuild-based Docker image builder for Lambda functions.\n+Offloads Docker builds to AWS for faster pulumi up times.\n+\n+Similar to EcsLambda but for Docker images:\n+- Fast `pulumi up` (async by default)\n+- Simple architecture: S3 ‚Üí CodePipeline ‚Üí CodeBuild ‚Üí ECR ‚Üí Lambda\n+- Maintains Pulumi state\n+- Supports multi-stage builds with layer caching\n+\"\"\"\n+\n+# pylint: disable=import-error\n+\n+import json\n+import shlex\n+from pathlib import Path\n+from typing import Any, Dict, Optional\n+\n+import pulumi\n+import pulumi_command as command\n+from pulumi import ComponentResource, Output, ResourceOptions\n+from pulumi_aws import config, get_caller_identity\n+from pulumi_aws.codebuild import (\n+    Project,\n+    ProjectArtifactsArgs,\n+    ProjectCacheArgs,\n+    ProjectEnvironmentArgs,\n+    ProjectEnvironmentEnvironmentVariableArgs,\n+    ProjectLogsConfigArgs,\n+    ProjectLogsConfigCloudwatchLogsArgs,\n+    ProjectSourceArgs,\n+)\n+from pulumi_aws.codepipeline import (\n+    Pipeline,\n+    PipelineArtifactStoreArgs,\n+    PipelineStageActionArgs,\n+    PipelineStageArgs,\n+)\n+from pulumi_aws.ecr import Repository, RepositoryImageScanningConfigurationArgs\n+from pulumi_aws.iam import Role as ROLE\n+from pulumi_aws.iam import RolePolicy\n+from pulumi_aws.lambda_ import (\n+    Function,\n+    FunctionEnvironmentArgs,\n+    FunctionEphemeralStorageArgs,\n+    FunctionFileSystemConfigArgs,\n+    FunctionImageConfigArgs,\n+    FunctionVpcConfigArgs,\n+)\n+\n+from infra.shared.build_utils import (\n+    compute_hash,\n+    make_artifact_bucket,\n+    make_log_group,\n+    resolve_build_config,\n+)\n+from infra.shared.buildspecs import docker_image_buildspec\n+from infra.utils import _find_project_root\n+\n+PROJECT_DIR = _find_project_root()\n+\n+\n+class CodeBuildDockerImage(ComponentResource):\n+    \"\"\"AWS CodeBuild-based Docker image builder with ECR push and Lambda update.\n+\n+    This component:\n+    - Creates ECR repository\n+    - Uploads Dockerfile + build context to S3\n+    - Runs CodeBuild to build multi-stage Docker image\n+    - Pushes to ECR with content-based tags\n+    - Updates Lambda function with new image URI\n+    - Fast pulumi up (async by default, sync in CI)\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        name: str,\n+        *,\n+        dockerfile_path: str,  # Path to Dockerfile relative to project root\n+        build_context_path: str,  # Path to build context (usually project root)\n+        source_paths: Optional[\n+            list[str]\n+        ] = None,  # Specific paths to include in build\n+        lambda_function_name: Optional[\n+            str\n+        ] = None,  # If provided, updates Lambda\n+        lambda_config: Optional[Dict[str, Any]] = None,  # Lambda configuration\n+        build_args: Optional[Dict[str, str]] = None,\n+        platform: str = \"linux/arm64\",\n+        sync_mode: Optional[bool] = None,\n+        lambda_aliases: Optional[\n+            list[str]\n+        ] = None,  # Pulumi aliases for Lambda rename\n+        opts: Optional[ResourceOptions] = None,\n+    ) -> None:\n+        super().__init__(f\"codebuild-docker:{name}\", name, {}, opts)\n+\n+        self.name = name\n+        # Keep paths relative - they'll be resolved from workspace root\n+        self.dockerfile_path = dockerfile_path\n+        self.build_context_path = build_context_path\n+        self.source_paths = (\n+            source_paths or []\n+        )  # Specific source paths for selective copying\n+        self.lambda_function_name = (\n+            lambda_function_name or f\"{name}-{pulumi.get_stack()}\"\n+        )\n+        self.lambda_config = lambda_config or {}\n+        self.build_args = build_args or {}\n+        self.platform = platform\n+        self.lambda_aliases = (\n+            lambda_aliases or []\n+        )  # Pulumi aliases for Lambda rename\n+\n+        # Configure build mode and flags\n+        (\n+            self.sync_mode,\n+            self.force_rebuild,\n+            self.debug_mode,\n+        ) = resolve_build_config(\n+            \"docker-build\",\n+            sync_override=sync_mode,\n+            ci_default_sync=True,\n+        )\n+\n+        # Calculate content hash for change detection\n+        content_hash = self._calculate_content_hash()\n+\n+        if self.sync_mode:\n+            pulumi.log.info(\n+                f\"üîÑ Building image '{self.name}' in SYNC mode (will wait)\"\n+            )\n+        else:\n+            pulumi.log.info(\n+                f\"‚ö° Image '{self.name}' in ASYNC mode (fast pulumi up)\"\n+            )\n+            pulumi.log.info(\n+                f\"   üì¶ Hash: {content_hash[:12]}... - will build only if changed\"\n+            )\n+\n+        # Create ECR repository\n+        self.ecr_repo = Repository(\n+            f\"{self.name}-repo\",\n+            image_scanning_configuration=RepositoryImageScanningConfigurationArgs(\n+                scan_on_push=True\n+            ),\n+            force_delete=True,\n+            opts=ResourceOptions(parent=self),\n+        )\n+\n+        # Setup build pipeline\n+        (\n+            _build_bucket,\n+            _upload_cmd,\n+            pipeline,\n+            _codebuild_project,\n+            pipeline_trigger_cmd,\n+        ) = self._setup_pipeline(content_hash)\n+\n+        # Push bootstrap image and create Lambda function if config provided\n+        if self.lambda_config:\n+            bootstrap_cmd = self._push_bootstrap_image()\n+            self._create_lambda_function(bootstrap_cmd, pipeline_trigger_cmd)\n+        else:\n+            self.lambda_function = None\n+\n+        # Export outputs\n+        self.repository_url = self.ecr_repo.repository_url\n+        self.image_uri = self.ecr_repo.repository_url.apply(\n+            lambda url: f\"{url}:latest\"\n+        )\n+        # Digest is managed by CodeBuild, provide a placeholder\n+        # The actual digest is used during Lambda updates in post_build phase\n+        self.digest = pulumi.Output.from_input(\"sha256:placeholder\")\n+\n+        # Self-reference for compatibility with components that wrap this\n+        self.docker_image = self\n+\n+        self.register_outputs(\n+            {\n+                \"repository_url\": self.repository_url,\n+                \"image_uri\": self.image_uri,\n+                \"digest\": self.digest,\n+            }\n+        )\n+\n+    def _calculate_content_hash(self) -> str:\n+        \"\"\"Calculate hash of Dockerfile and relevant context files.\"\"\"\n+        paths: list[Path] = []\n+\n+        # Include Dockerfile\n+        dockerfile = Path(PROJECT_DIR) / self.dockerfile_path\n+        if dockerfile.exists():\n+            paths.append(dockerfile)\n+\n+        # If source_paths specified, hash only those paths\n+        if self.source_paths:\n+            for source_path in sorted(self.source_paths):\n+                full_path = Path(PROJECT_DIR) / source_path\n+                if full_path.exists():\n+                    paths.append(full_path)\n+\n+            # ALWAYS hash the handler directory (Lambda-specific code)\n+            handler_dir = Path(PROJECT_DIR) / Path(self.dockerfile_path).parent\n+            if handler_dir.exists():\n+                for file_path in sorted(handler_dir.rglob(\"*.py\")):\n+                    if file_path.is_file() and not any(\n+                        part.startswith(\".\") or part == \"__pycache__\"\n+                        for part in file_path.parts\n+                    ):\n+                        paths.append(file_path)\n+        else:\n+            # Hash only the files that will be included in the build context\n+            if self.build_context_path == \".\":\n+                # Lambda images - hash Python packages and handler directory\n+                # Default packages that all Lambda images need\n+                packages_to_hash = [\n+                    \"receipt_dynamo/receipt_dynamo\",\n+                    \"receipt_dynamo/pyproject.toml\",\n+                    \"receipt_chroma/receipt_chroma\",\n+                    \"receipt_chroma/pyproject.toml\",\n+                    \"receipt_label/receipt_label\",\n+                    \"receipt_label/pyproject.toml\",\n+                ]\n+\n+                for package_path in packages_to_hash:\n+                    full_path = Path(PROJECT_DIR) / package_path\n+                    if full_path.exists():\n+                        paths.append(full_path)\n+\n+                # Also hash the handler directory\n+                handler_dir = (\n+                    Path(PROJECT_DIR) / Path(self.dockerfile_path).parent\n+                )\n+                if handler_dir.exists():\n+                    for file_path in sorted(handler_dir.rglob(\"*.py\")):\n+                        if file_path.is_file() and not any(\n+                            part.startswith(\".\") or part == \"__pycache__\"\n+                            for part in file_path.parts\n+                        ):\n+                            paths.append(file_path)\n+            else:\n+                # ECS images - hash the specific context directory\n+                context_path = Path(PROJECT_DIR) / self.build_context_path\n+                if context_path.exists():\n+                    paths.append(context_path)\n+\n+        return compute_hash(\n+            paths,\n+            include_globs=[\n+                \"**/*.py\",\n+                \"**/pyproject.toml\",\n+                \"**/requirements.txt\",\n+                \"Dockerfile\",\n+            ],\n+        )\n+\n+    def _generate_upload_script(self, bucket: str, content_hash: str) -> str:\n+        \"\"\"Generate script to upload build context to S3.\"\"\"\n+        safe_bucket = shlex.quote(bucket)\n+\n+        # Build source paths string for script\n+        source_paths_str = \"\"\n+        if self.source_paths:\n+            # Convert paths to space-separated string\n+            paths = \" \".join(shlex.quote(p) for p in self.source_paths)\n+            source_paths_str = paths\n+\n+        # Paths are relative to project root\n+        # Get absolute project root path\n+        project_root_abs = str(Path(PROJECT_DIR).resolve())\n+        safe_project_root = shlex.quote(project_root_abs)\n+        safe_context = shlex.quote(self.build_context_path)\n+        safe_dockerfile = shlex.quote(self.dockerfile_path)\n+\n+        return f\"\"\"#!/usr/bin/env bash\n+set -e\n+\n+# Change to project root\n+cd {safe_project_root}\n+\n+BUCKET={safe_bucket}\n+CONTEXT_PATH={safe_context}\n+DOCKERFILE={safe_dockerfile}\n+HASH=\"{content_hash}\"\n+NAME=\"{self.name}\"\n+FORCE_REBUILD=\"{self.force_rebuild}\"\n+SOURCE_PATHS=\"{source_paths_str}\"\n+\n+echo \"üì¶ Checking if context upload needed for image '$NAME'...\"\n+STORED_HASH=$(aws s3 cp \"s3://$BUCKET/$NAME/hash.txt\" - 2>/dev/null || echo '')\n+if [ \"$STORED_HASH\" = \"$HASH\" ] && [ \"$FORCE_REBUILD\" != \"True\" ]; then\n+  echo \"‚úÖ Context up-to-date. Skipping upload.\"\n+  exit 0\n+fi\n+\n+TMP=$(mktemp -d)\n+trap 'rm -rf \"$TMP\"' EXIT\n+\n+echo \"üì¶ Preparing build context...\"\n+mkdir -p \"$TMP/context\"\n+\n+# Copy only required files using include patterns\n+echo \"üì¶ Copying minimal context with include patterns...\"\n+\n+if [ \"$CONTEXT_PATH\" = \".\" ]; then\n+  # Lambda images - need packages from monorepo root\n+  # Default packages that all Lambda images need\n+  echo \"  ‚Üí Including receipt_dynamo, receipt_chroma, and receipt_label packages...\"\n+  rsync -a \\\n+    --include='receipt_dynamo/' \\\n+    --include='receipt_dynamo/pyproject.toml' \\\n+    --include='receipt_dynamo/receipt_dynamo/' \\\n+    --include='receipt_dynamo/receipt_dynamo/**' \\\n+    --include='receipt_dynamo/docs/' \\\n+    --include='receipt_dynamo/docs/README.md' \\\n+    --include='receipt_chroma/' \\\n+    --include='receipt_chroma/pyproject.toml' \\\n+    --include='receipt_chroma/README.md' \\\n+    --include='receipt_chroma/receipt_chroma/' \\\n+    --include='receipt_chroma/receipt_chroma/**' \\\n+    --exclude='receipt_chroma/__pycache__/' \\\n+    --exclude='receipt_chroma/**/__pycache__/' \\\n+    --exclude='receipt_chroma/tests/' \\\n+    --exclude='receipt_chroma/tests/**' \\\n+    --exclude='receipt_chroma/venv/' \\\n+    --exclude='receipt_chroma/venv/**' \\\n+    --exclude='receipt_chroma/htmlcov/' \\\n+    --exclude='receipt_chroma/htmlcov/**' \\\n+    --exclude='receipt_chroma/*.md' \\\n+    --exclude='receipt_chroma/conftest.py' \\\n+    --exclude='receipt_chroma/coverage.json' \\\n+    --include='receipt_label/' \\\n+    --include='receipt_label/pyproject.toml' \\\n+    --include='receipt_label/receipt_label/' \\\n+    --include='receipt_label/receipt_label/**' \\\n+    --include='receipt_label/README.md' \\\n+    --include='receipt_label/LICENSE' \\\n+    --exclude='*' \\\n+    \"$CONTEXT_PATH/\" \"$TMP/context/\"\n+\n+  # Copy additional source paths if specified (e.g., receipt_upload)\n+  if [ -n \"$SOURCE_PATHS\" ]; then\n+    echo \"  ‚Üí Including additional source paths: $SOURCE_PATHS\"\n+    for SOURCE_PATH in $SOURCE_PATHS; do\n+      if [ -d \"$SOURCE_PATH\" ]; then\n+        rsync -a \\\n+          --exclude='__pycache__' \\\n+          --exclude='*.pyc' \\\n+          --exclude='.git' \\\n+          \"$SOURCE_PATH/\" \"$TMP/context/$SOURCE_PATH/\"\n+      fi\n+    done\n+  fi","path":"infra/components/codebuild_docker_image.py","commit_id":"b4b5b64f14380322d789450da96a71cea99e1006","original_commit_id":"b4b5b64f14380322d789450da96a71cea99e1006","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Quoted `SOURCE_PATHS` prevents additional source paths from being copied into the build context.**\n\n`_generate_upload_script` builds `SOURCE_PATHS` via `shlex.quote` and then wraps it in double quotes:\n\n```bash\nSOURCE_PATHS=\"'receipt_upload'\"\n...\nfor SOURCE_PATH in $SOURCE_PATHS; do\n  if [ -d \"$SOURCE_PATH\" ]; then\n    rsync ...\n  fi\ndone\n```\n\nThe resulting values include literal single quotes, so `[ -d \"$SOURCE_PATH\" ]` fails and the extra paths are never rsynced. For images relying on these extra paths (e.g. `receipt_upload`), their code never reaches the Docker build context.\n\nYou can fix this by dropping the inner `shlex.quote` and joining the relative paths directly (they are already under your control and don‚Äôt contain spaces):\n\n```diff\n-        # Build source paths string for script\n-        source_paths_str = \"\"\n-        if self.source_paths:\n-            # Convert paths to space-separated string\n-            paths = \" \".join(shlex.quote(p) for p in self.source_paths)\n-            source_paths_str = paths\n+        # Build source paths string for script\n+        source_paths_str = \"\"\n+        if self.source_paths:\n+            # Paths are relative and expected not to contain spaces; join directly\n+            source_paths_str = \" \".join(self.source_paths)\n...\n-SOURCE_PATHS=\"{source_paths_str}\"\n+SOURCE_PATHS=\"{source_paths_str}\"\n```\n\nThis preserves proper splitting in `for SOURCE_PATH in $SOURCE_PATHS` and ensures the additional source directories are actually copied.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\ninfra/components/codebuild_docker_image.py lines 265-357: the generated\nSOURCE_PATHS contains embedded single quotes because each path is wrapped with\nshlex.quote, preventing the shell loop from seeing valid directory names; fix it\nby building source_paths_str using plain join of the relative paths (no\nshlex.quote) so SOURCE_PATHS receives unquoted space-separated paths (they are\ncontrolled and contain no spaces), leaving the assignment\nSOURCE_PATHS=\"{source_paths_str}\" as-is so the for SOURCE_PATH in $SOURCE_PATHS\nloop can iterate correctly and rsync the additional directories.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:olive -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-12-06T03:48:06Z","updated_at":"2025-12-06T03:48:06Z","html_url":"https://github.com/tnorlund/Portfolio/pull/491#discussion_r2594486448","pull_request_url":"https://api.github.com/repos/tnorlund/Portfolio/pulls/491","_links":{"self":{"href":"https://api.github.com/repos/tnorlund/Portfolio/pulls/comments/2594486448"},"html":{"href":"https://github.com/tnorlund/Portfolio/pull/491#discussion_r2594486448"},"pull_request":{"href":"https://api.github.com/repos/tnorlund/Portfolio/pulls/491"}},"reactions":{"url":"https://api.github.com/repos/tnorlund/Portfolio/pulls/comments/2594486448/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":265,"original_start_line":265,"start_side":"RIGHT","line":357,"original_line":357,"side":"RIGHT","author_association":"NONE","original_position":357,"position":357,"subject_type":"line"},{"url":"https://api.github.com/repos/tnorlund/Portfolio/pulls/comments/2594486449","pull_request_review_id":3547107753,"id":2594486449,"node_id":"PRRC_kwDOM8BQvs6apLix","diff_hunk":"@@ -0,0 +1,805 @@\n+\"\"\"\n+ecs_lambda.py\n+\n+Hybrid Lambda Function component similar to lambda_layer.py, but for Lambda code.\n+\n+Goals:\n+- Fast `pulumi up` (async by default) by offloading packaging to AWS CodeBuild\n+- Simple architecture: S3 (source), CodePipeline ‚Üí CodeBuild ‚Üí update Lambda code\n+- Optionally wait in sync mode (useful in CI) to ensure function updated\n+- Supports including local `receipt-*` monorepo packages into the build\n+- Avoids Step Functions/SQS; keeps infra simple and debuggable\n+\n+Usage pattern mirrors `LambdaLayer` but targets function packaging and updates.\n+\"\"\"\n+\n+# pylint: disable=import-error\n+\n+import base64\n+import glob\n+import json\n+import os\n+import shlex\n+from typing import Any, Dict, List, Optional\n+\n+import pulumi\n+import pulumi_command as command\n+from pulumi import (\n+    AssetArchive,\n+    ComponentResource,\n+    Output,\n+    ResourceOptions,\n+    StringAsset,\n+)\n+from pulumi_aws import config, get_caller_identity\n+from pulumi_aws.codebuild import (\n+    Project,\n+    ProjectArtifactsArgs,\n+    ProjectEnvironmentArgs,\n+    ProjectEnvironmentEnvironmentVariableArgs,\n+    ProjectLogsConfigArgs,\n+    ProjectLogsConfigCloudwatchLogsArgs,\n+    ProjectSourceArgs,\n+)\n+from pulumi_aws.codepipeline import (\n+    Pipeline,\n+    PipelineArtifactStoreArgs,\n+    PipelineStageActionArgs,\n+    PipelineStageArgs,\n+)\n+from pulumi_aws.iam import Role as ROLE\n+from pulumi_aws.iam import RolePolicy, RolePolicyAttachment\n+from pulumi_aws.lambda_ import Function, FunctionEnvironmentArgs\n+\n+from infra.shared.build_utils import (\n+    compute_hash,\n+    make_artifact_bucket,\n+    make_log_group,\n+)\n+from infra.utils import _find_project_root\n+\n+PROJECT_DIR = _find_project_root()\n+\n+\n+class EcsLambda(ComponentResource):\n+    \"\"\"AWS-offloaded Lambda function builder/deployer using CodeBuild/CodePipeline.\n+\n+    This component handles:\n+    - Zipping your sources (with optional local monorepo deps) and uploading to S3\n+    - Running CodeBuild to assemble `function.zip` (pip installs, optional wheels)\n+    - Updating the Lambda code with `aws lambda update-function-code`\n+\n+    By default, `pulumi up` is fast (async). For CI/CD, enable sync to wait.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        name: str,\n+        *,\n+        package_dir: str,\n+        handler: str,\n+        python_version: str = \"3.12\",\n+        description: Optional[str] = None,\n+        role_arn: Optional[Output[str] | str] = None,\n+        timeout: int = 30,\n+        memory_size: int = 512,\n+        environment: Optional[Dict[str, str]] = None,\n+        layers: Optional[List[Output[str] | str]] = None,\n+        package_extras: Optional[\n+            str\n+        ] = None,  # e.g., \"lambda\" to install `pkg[lambda]`\n+        sync_mode: Optional[bool] = None,\n+        opts: Optional[ResourceOptions] = None,\n+    ) -> None:\n+        super().__init__(f\"ecs-lambda:{name}\", name, {}, opts)\n+\n+        self.name = name\n+        self.package_dir = package_dir\n+        self.handler = handler  # e.g., \"my_module.lambda_handler\"\n+        self.python_version = python_version\n+        self.description = (\n+            description or f\"AWS-built Lambda function for {name}\"\n+        )\n+        self.role_arn = role_arn\n+        self.timeout = timeout\n+        self.memory_size = memory_size\n+        self.environment = environment or {}\n+        self.layers = layers or []\n+        self.package_extras = package_extras\n+\n+        # Configure build mode\n+        if sync_mode is not None:\n+            self.sync_mode = sync_mode\n+        else:\n+            ecs_config = pulumi.Config(\"ecs-lambda\")\n+            if ecs_config.get_bool(\"sync-mode\"):\n+                self.sync_mode = True\n+            elif os.getenv(\"CI\") or os.getenv(\"GITHUB_ACTIONS\"):\n+                self.sync_mode = True\n+            else:\n+                self.sync_mode = False\n+\n+        # Additional config flags\n+        ecs_config = pulumi.Config(\"ecs-lambda\")\n+        self.force_rebuild = ecs_config.get_bool(\"force-rebuild\") or False\n+        self.debug_mode = ecs_config.get_bool(\"debug-mode\") or False\n+\n+        # Validate input source directory and compute change hash\n+        self._validate_package_dir()\n+        self.package_path = os.path.join(PROJECT_DIR, self.package_dir)\n+        package_hash = self._calculate_package_hash()\n+\n+        if self.sync_mode:\n+            pulumi.log.info(\n+                f\"üîÑ Building function '{self.name}' in SYNC mode (will wait)\"\n+            )\n+        else:\n+            pulumi.log.info(\n+                f\"‚ö° Function '{self.name}' in ASYNC mode (fast pulumi up)\"\n+            )\n+            if self.force_rebuild:\n+                pulumi.log.info(\n+                    \"   üî® Force rebuild enabled - will trigger build\"\n+                )\n+            else:\n+                pulumi.log.info(\n+                    f\"   üì¶ Hash: {package_hash[:12]}... - will build only if changed\"\n+                )\n+\n+        # Infra setup\n+        (\n+            _build_bucket,\n+            upload_cmd,\n+            pipeline,\n+            _publish_project,\n+            _codebuild_role,\n+        ) = self._setup_pipeline(package_hash)\n+\n+        # Create the Lambda function (with small placeholder code), then let pipeline update code\n+        self.function_name = f\"{self.name}-{pulumi.get_stack()}\"\n+        placeholder_code = self._make_placeholder_code(self.handler)\n+\n+        # Lambda role: allow passing explicit role; otherwise create a basic one\n+        if self.role_arn is None:\n+            assume = json.dumps(\n+                {\n+                    \"Version\": \"2012-10-17\",\n+                    \"Statement\": [\n+                        {\n+                            \"Effect\": \"Allow\",\n+                            \"Principal\": {\"Service\": \"lambda.amazonaws.com\"},\n+                            \"Action\": \"sts:AssumeRole\",\n+                        }\n+                    ],\n+                }\n+            )\n+            role = ROLE(\n+                f\"{self.name}-lambda-role\",\n+                assume_role_policy=assume,\n+                opts=pulumi.ResourceOptions(parent=self),\n+            )\n+            RolePolicyAttachment(\n+                f\"{self.name}-lambda-basic-exec\",\n+                role=role.name,\n+                policy_arn=\"arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\",\n+                opts=pulumi.ResourceOptions(parent=self),\n+            )\n+            self.role_arn = role.arn\n+\n+        # Create function with placeholder code; ignore code updates from Pulumi afterwards\n+        self.function = Function(\n+            f\"{self.name}-function\",\n+            name=self.function_name,\n+            runtime=f\"python{self.python_version}\",\n+            role=self.role_arn,\n+            handler=self.handler,\n+            code=placeholder_code,\n+            timeout=self.timeout,\n+            memory_size=self.memory_size,\n+            environment=(\n+                FunctionEnvironmentArgs(variables=self.environment)\n+                if self.environment\n+                else None\n+            ),\n+            layers=self.layers if self.layers else None,\n+            description=self.description,\n+            opts=pulumi.ResourceOptions(\n+                parent=self,\n+                ignore_changes=[\"code\", \"image_uri\", \"package_type\"],\n+                depends_on=[upload_cmd, pipeline],\n+            ),\n+        )\n+\n+        # Optionally trigger pipeline in async mode\n+        if not self.sync_mode:\n+            trigger_script = pipeline.name.apply(\n+                lambda pn: f\"\"\"#!/usr/bin/env bash\n+set -e\n+echo \"üîÑ Changes detected, starting CodePipeline execution for {self.name}\"\n+EXEC_ID=$(aws codepipeline start-pipeline-execution --name {pn} \\\n+  --query pipelineExecutionId --output text)\n+echo \"Triggered pipeline: $EXEC_ID\"\n+\"\"\"\n+            )\n+            command.local.Command(\n+                f\"{self.name}-trigger-pipeline\",\n+                create=trigger_script,\n+                update=trigger_script,\n+                triggers=[package_hash],\n+                opts=pulumi.ResourceOptions(\n+                    parent=self, depends_on=[upload_cmd, pipeline]\n+                ),\n+            )\n+        else:\n+            # Sync: run pipeline and wait for completion before finishing\n+            sync_script = pipeline.name.apply(\n+                lambda pn: f\"\"\"#!/usr/bin/env bash\n+set -e\n+echo \"üîÑ Sync: Starting CodePipeline execution for {self.name}\"\n+EXEC_ID=$(aws codepipeline start-pipeline-execution --name {pn} \\\n+  --query pipelineExecutionId --output text)\n+echo \"Execution ID: $EXEC_ID\"\n+sleep 2\n+while true; do\n+  STATUS=$(aws codepipeline get-pipeline-execution --pipeline-name {pn} \\\n+    --pipeline-execution-id $EXEC_ID \\\n+    --query \"pipelineExecution.status\" --output text)\n+  echo \"üîÑ Pipeline status: $STATUS\"\n+  if [ \"$STATUS\" = \"Succeeded\" ]; then\n+    echo \"‚úÖ Pipeline completed successfully\"\n+    break\n+  elif [ \"$STATUS\" = \"Failed\" ] || [ \"$STATUS\" = \"Superseded\" ]; then\n+    echo \"‚ùå Pipeline failed with status: $STATUS\"\n+    exit 1\n+  fi\n+  sleep 10\n+done\n+\"\"\"\n+            )\n+            command.local.Command(\n+                f\"{self.name}-sync-pipeline\",\n+                create=sync_script,\n+                update=sync_script,\n+                triggers=[package_hash],\n+                opts=pulumi.ResourceOptions(\n+                    parent=self, depends_on=[upload_cmd, pipeline]\n+                ),\n+            )\n+\n+        # Export useful outputs\n+        self.arn = self.function.arn\n+        self.invoke_arn = self.function.invoke_arn\n+\n+    # ---------------------------- Helpers ---------------------------- #\n+\n+    def _validate_package_dir(self) -> None:\n+        src_path = os.path.join(PROJECT_DIR, self.package_dir)\n+        if not os.path.exists(src_path):\n+            raise ValueError(f\"Package directory {src_path} does not exist\")\n+        # Accept any Python files; `pyproject.toml` and/or `requirements.txt` are optional\n+        py_files = glob.glob(os.path.join(src_path, \"**/*.py\"), recursive=True)\n+        if not py_files:\n+            raise ValueError(\n+                f\"Package directory {src_path} contains no Python files\"\n+            )\n+\n+    def _calculate_package_hash(self) -> str:\n+        src_path = os.path.join(PROJECT_DIR, self.package_dir)\n+        return compute_hash(\n+            [src_path],\n+            include_globs=[\"**/*.py\", \"pyproject.toml\", \"requirements.txt\"],\n+            extra_strings={\"package_dir\": self.package_dir},\n+        )\n+\n+    def _get_local_dependencies(self) -> List[str]:\n+        \"\"\"Find monorepo local packages referenced in pyproject optional extras.\n+\n+        Matches `receipt-*` and converts to `_` folder names (e.g., receipt-label ‚Üí receipt_label).\n+        \"\"\"\n+        src_path = os.path.join(PROJECT_DIR, self.package_dir)\n+        pyproject_path = os.path.join(src_path, \"pyproject.toml\")\n+        local_deps: List[str] = []\n+        if os.path.exists(pyproject_path):\n+            try:\n+                try:\n+                    import tomllib  # pyright: ignore[reportMissingImports]\n+\n+                    with open(pyproject_path, \"rb\") as f:\n+                        data = tomllib.load(f)\n+                except Exception:\n+                    import toml  # type: ignore\n+\n+                    with open(pyproject_path, \"r\") as f:\n+                        data = toml.load(f)\n+\n+                deps = data.get(\"project\", {}).get(\"dependencies\", [])\n+                if self.package_extras:\n+                    optional = data.get(\"project\", {}).get(\n+                        \"optional-dependencies\", {}\n+                    )\n+                    deps.extend(optional.get(self.package_extras, []))\n+\n+                for dep in deps:\n+                    dep_name = (\n+                        dep.split(\"[\")[0]\n+                        .split(\">\")[0]\n+                        .split(\"<\")[0]\n+                        .split(\"=\")[0]\n+                        .strip()\n+                    )\n+                    if dep_name.startswith(\"receipt-\"):\n+                        dir_name = dep_name.replace(\"-\", \"_\")\n+                        local_path = os.path.join(PROJECT_DIR, dir_name)\n+                        if os.path.exists(local_path):\n+                            local_deps.append(dir_name)\n+                            pulumi.log.info(\n+                                f\"üì¶ Found local dependency: {dir_name} for {self.name}\"\n+                            )\n+            except Exception as e:  # pylint: disable=broad-exception-caught\n+                pulumi.log.warn(\n+                    f\"Could not parse pyproject.toml for deps: {e}\"\n+                )\n+        return local_deps\n+\n+    def _encode(self, script: str) -> str:\n+        return base64.b64encode(script.encode(\"utf-8\")).decode(\"utf-8\")\n+\n+    def _generate_upload_script(self, bucket: str, package_hash: str) -> str:\n+        safe_bucket = shlex.quote(bucket)\n+        safe_src = shlex.quote(self.package_path)\n+        safe_project_root = shlex.quote(str(PROJECT_DIR))\n+        local_deps = self._get_local_dependencies()\n+        return f\"\"\"#!/usr/bin/env bash\n+set -e\n+\n+BUCKET={safe_bucket}\n+SRC={safe_src}\n+HASH=\"{package_hash}\"\n+NAME=\"{self.name}\"\n+FORCE_REBUILD=\"{self.force_rebuild}\"\n+LOCAL_DEPS=\"{' '.join(local_deps)}\"\n+BASE_DIR={safe_project_root}\n+\n+echo \"üì¶ Checking if source upload needed for function '$NAME'...\"\n+STORED_HASH=$(aws s3 cp \"s3://$BUCKET/$NAME/hash.txt\" - 2>/dev/null || echo '')\n+if [ \"$STORED_HASH\" = \"$HASH\" ] && [ \"$FORCE_REBUILD\" != \"True\" ]; then\n+  echo \"‚úÖ Source up-to-date. Skipping upload.\"\n+  exit 0\n+fi\n+\n+TMP=$(mktemp -d)\n+trap 'rm -rf \"$TMP\"' EXIT\n+mkdir -p \"$TMP/source\"\n+cp -r \"$SRC\"/* \"$TMP/source/\"\n+\n+if [ -n \"$LOCAL_DEPS\" ]; then\n+  echo \"Including local dependencies: $LOCAL_DEPS\"\n+  mkdir -p \"$TMP/dependencies\"\n+  for dep in $LOCAL_DEPS; do\n+    DEP_PATH=\"$BASE_DIR/$dep\"\n+    if [ -d \"$DEP_PATH\" ]; then\n+      mkdir -p \"$TMP/dependencies/$dep\"\n+      cp -r \"$DEP_PATH\"/* \"$TMP/dependencies/$dep/\"\n+    fi\n+  done\n+fi\n+\n+cd \"$TMP\"\n+if [ -d dependencies ]; then\n+  zip -qr source.zip source dependencies\n+else\n+  zip -qr source.zip source\n+fi\n+cd - >/dev/null\n+\n+aws s3 cp \"$TMP/source.zip\" \"s3://$BUCKET/$NAME/source.zip\"\n+echo -n \"$HASH\" | aws s3 cp - \"s3://$BUCKET/$NAME/hash.txt\"\n+echo \"‚úÖ Uploaded source.zip\"\n+\"\"\"\n+\n+    def _make_placeholder_code(self, handler: str) -> AssetArchive:\n+        # Create module file path and function name for the configured handler\n+        # Lambda handler is usually \"module.function\"; support dotted module path\n+        parts = handler.split(\".\")\n+        if len(parts) < 2:\n+            module_path = \"index\"\n+            func_name = handler\n+        else:\n+            module_path = \"/\".join(parts[:-1])\n+            func_name = parts[-1]\n+        file_path = f\"{module_path}.py\" if module_path else \"index.py\"\n+        content = (\n+            \"import json\\n\"\n+            \"def {}(event, context):\\n\"\n+            \"    return {{'statusCode': 200, 'body': \"\n+            \"json.dumps({{'message': 'placeholder'}})}}\\n\".format(func_name)\n+        )\n+        return AssetArchive({file_path: StringAsset(content)})\n+\n+    def _buildspec(self) -> Dict[str, Any]:\n+        v = self.python_version\n+        extras = f\"[{self.package_extras}]\" if self.package_extras else \"\"\n+        cmds: List[str] = []\n+        cmds.extend(\n+            [\n+                \"echo Preparing function build...\",\n+                \"rm -rf build && mkdir -p build/package\",\n+                # Debug\n+                '[ \"$DEBUG_MODE\" = \"True\" ] && echo \"DEBUG: Listing source\" && ls -la || true',\n+                # Build local dependencies first if present\n+                (\n+                    'if [ -d \"dependencies\" ]; then '\n+                    'echo \"Building local dependency wheels...\"; '\n+                    \"mkdir -p dep_wheels; \"\n+                    f'for d in dependencies/*; do if [ -d \"$d\" ]; then cd \"$d\"; '\n+                    f\"python{v} -m build --wheel --outdir ../../dep_wheels/; \"\n+                    \"cd - >/dev/null; fi; done; \"\n+                    \"fi\"\n+                ),\n+                # Main install strategy: pyproject wheel if exists, else requirements.txt\n+                (\n+                    'if [ -f \"source/pyproject.toml\" ]; then '\n+                    'echo \"Building source wheel\"; '\n+                    f\"cd source && python{v} -m build --wheel --outdir ../dist/ && cd ..; \"\n+                    # Install built wheel with optional extras and local wheels first\n+                    f\"WHEEL=$(ls dist/*.whl | head -1); \"\n+                    f\"python{v} -m pip install --no-cache-dir --find-links dep_wheels \"\n+                    f'\"$WHEEL{extras}\" -t build/package || '\n+                    f\"python{v} -m pip install --no-cache-dir --find-links dep_wheels \"\n+                    \"dist/*.whl -t build/package; \"\n+                    'elif [ -f \"source/requirements.txt\" ]; then '\n+                    'echo \"Installing from requirements.txt (using local wheels if present)\"; '\n+                    f\"python{v} -m pip install --no-cache-dir --find-links dep_wheels \"\n+                    \"-r source/requirements.txt -t build/package; \"\n+                    \"fi\"\n+                ),\n+                # Copy source files last to ensure they override site-packages collisions if any\n+                'echo \"Copying source files\"',\n+                \"cp -r source/. build/package/\",\n+                # Cleanup common large libs provided by Lambda runtime or not needed\n+                (\n+                    \"find build/package -type d -name '__pycache__' -exec rm -rf {} + \"\n+                    \"2>/dev/null || true\"\n+                ),\n+                # Zip\n+                \"cd build/package && zip -qr ../function.zip . && cd - >/dev/null\",\n+                # Upload to S3 and update Lambda\n+                'echo \"Uploading function.zip to S3\"',\n+                (\n+                    \"aws s3 cp build/function.zip \"\n+                    \"s3://$BUCKET_NAME/$PACKAGE_NAME/combined/function.zip\"\n+                ),\n+                'echo \"Updating Lambda function code\"',\n+                (\n+                    'aws lambda update-function-code --function-name \"$FUNCTION_NAME\" '\n+                    '--s3-bucket \"$BUCKET_NAME\" --s3-key \"$PACKAGE_NAME/combined/function.zip\" '\n+                    \">/dev/null\"\n+                ),\n+                'echo \"Lambda code update completed\"',\n+            ]\n+        )\n+        return {\n+            \"version\": 0.2,\n+            \"phases\": {\n+                \"install\": {\n+                    \"runtime-versions\": {\"python\": v},\n+                    \"commands\": [\n+                        f\"python{v} -m pip install --upgrade pip build\",\n+                        \"set -e\",\n+                    ],\n+                },\n+                \"build\": {\"commands\": cmds},\n+            },\n+            \"artifacts\": {\"files\": [\"build/function.zip\"]},\n+        }\n+\n+    def _setup_pipeline(self, package_hash: str):\n+        # Artifact bucket\n+        build_bucket, _bucket_versioning = make_artifact_bucket(\n+            f\"{self.name}-fn\", parent=self\n+        )\n+\n+        # Upload source command (idempotent thanks to hash tracking)\n+        upload_cmd = command.local.Command(\n+            f\"{self.name}-fn-upload-source\",\n+            create=build_bucket.bucket.apply(\n+                lambda b: self._generate_upload_script(b, package_hash)\n+            ),\n+            update=build_bucket.bucket.apply(\n+                lambda b: self._generate_upload_script(b, package_hash)\n+            ),\n+            triggers=[package_hash],\n+            opts=pulumi.ResourceOptions(\n+                parent=self, delete_before_replace=True\n+            ),\n+        )\n+\n+        # IAM role for CodeBuild/CodePipeline\n+        codebuild_role = ROLE(\n+            f\"{self.name}-fn-codebuild-role\",\n+            assume_role_policy=json.dumps(\n+                {\n+                    \"Version\": \"2012-10-17\",\n+                    \"Statement\": [\n+                        {\n+                            \"Effect\": \"Allow\",\n+                            \"Principal\": {\n+                                \"Service\": \"codebuild.amazonaws.com\"\n+                            },\n+                            \"Action\": \"sts:AssumeRole\",\n+                        },\n+                        {\n+                            \"Effect\": \"Allow\",\n+                            \"Principal\": {\n+                                \"Service\": \"codepipeline.amazonaws.com\"\n+                            },\n+                            \"Action\": \"sts:AssumeRole\",\n+                        },\n+                    ],\n+                }\n+            ),\n+            opts=pulumi.ResourceOptions(parent=self),\n+        )\n+\n+        RolePolicy(\n+            f\"{self.name}-fn-codebuild-policy\",\n+            role=codebuild_role.id,\n+            policy=pulumi.Output.all(build_bucket.arn, self.name).apply(\n+                lambda args: json.dumps(\n+                    {\n+                        \"Version\": \"2012-10-17\",\n+                        \"Statement\": [\n+                            {\n+                                \"Effect\": \"Allow\",\n+                                \"Action\": [\n+                                    \"logs:CreateLogGroup\",\n+                                    \"logs:CreateLogStream\",\n+                                    \"logs:PutLogEvents\",\n+                                ],\n+                                \"Resource\": [\n+                                    (\n+                                        f\"arn:aws:logs:{config.region}:\"\n+                                        f\"{get_caller_identity().account_id}:\"\n+                                        \"log-group:/aws/codebuild/*\"\n+                                    ),\n+                                    (\n+                                        f\"arn:aws:logs:{config.region}:\"\n+                                        f\"{get_caller_identity().account_id}:\"\n+                                        \"log-group:/aws/codebuild/*:*\"\n+                                    ),\n+                                ],\n+                            },\n+                            {\n+                                \"Effect\": \"Allow\",\n+                                \"Action\": [\n+                                    \"s3:GetObject\",\n+                                    \"s3:GetObjectVersion\",\n+                                    \"s3:PutObject\",\n+                                ],\n+                                \"Resource\": f\"{args[0]}/*\",\n+                            },\n+                            {\n+                                \"Effect\": \"Allow\",\n+                                \"Action\": [\n+                                    \"s3:GetBucketAcl\",\n+                                    \"s3:GetBucketLocation\",\n+                                    \"s3:ListBucket\",\n+                                ],\n+                                \"Resource\": args[0],\n+                            },\n+                            {\n+                                \"Effect\": \"Allow\",\n+                                \"Action\": [\n+                                    \"lambda:UpdateFunctionCode\",\n+                                    \"lambda:UpdateFunctionConfiguration\",\n+                                    \"lambda:GetFunctionConfiguration\",\n+                                    \"lambda:ListTags\",\n+                                ],\n+                                \"Resource\": (\n+                                    f\"arn:aws:lambda:{config.region}:\"\n+                                    f\"{get_caller_identity().account_id}:function:\"\n+                                    f\"{self.function_name}\"\n+                                ),\n+                            },\n+                            {\n+                                \"Effect\": \"Allow\",\n+                                \"Action\": [\n+                                    \"codebuild:StartBuild\",\n+                                    \"codebuild:BatchGetBuilds\",\n+                                ],\n+                                \"Resource\": (\n+                                    f\"arn:aws:codebuild:{config.region}:\"\n+                                    f\"{get_caller_identity().account_id}:project/*\"\n+                                ),\n+                            },\n+                        ],\n+                    }\n+                )\n+            ),\n+            opts=pulumi.ResourceOptions(parent=self),\n+        )\n+\n+        pipeline_role = ROLE(\n+            f\"{self.name}-fn-pipeline-role\",\n+            assume_role_policy=json.dumps(\n+                {\n+                    \"Version\": \"2012-10-17\",\n+                    \"Statement\": [\n+                        {\n+                            \"Effect\": \"Allow\",\n+                            \"Principal\": {\n+                                \"Service\": \"codepipeline.amazonaws.com\"\n+                            },\n+                            \"Action\": \"sts:AssumeRole\",\n+                        }\n+                    ],\n+                }\n+            ),\n+            opts=pulumi.ResourceOptions(parent=self),\n+        )\n+\n+        RolePolicy(\n+            f\"{self.name}-fn-pipeline-s3\",\n+            role=pipeline_role.id,\n+            policy=build_bucket.arn.apply(\n+                lambda arn: json.dumps(\n+                    {\n+                        \"Version\": \"2012-10-17\",\n+                        \"Statement\": [\n+                            {\n+                                \"Effect\": \"Allow\",\n+                                \"Action\": [\n+                                    \"s3:ListBucket\",\n+                                    \"s3:GetBucketLocation\",\n+                                    \"s3:GetBucketVersioning\",\n+                                ],\n+                                \"Resource\": arn,\n+                            },\n+                            {\n+                                \"Effect\": \"Allow\",\n+                                \"Action\": [\n+                                    \"s3:GetObject\",\n+                                    \"s3:GetObjectVersion\",\n+                                    \"s3:PutObject\",\n+                                ],\n+                                \"Resource\": f\"{arn}/{self.name}/*\",\n+                            },\n+                        ],\n+                    }\n+                )\n+            ),\n+            opts=pulumi.ResourceOptions(parent=self),\n+        )\n+\n+        RolePolicy(\n+            f\"{self.name}-fn-pipeline-codebuild\",\n+            role=pipeline_role.id,\n+            policy=Output.all(\n+                config.region, get_caller_identity().account_id\n+            ).apply(\n+                lambda args: json.dumps(\n+                    {\n+                        \"Version\": \"2012-10-17\",\n+                        \"Statement\": [\n+                            {\n+                                \"Effect\": \"Allow\",\n+                                \"Action\": [\n+                                    \"codebuild:StartBuild\",\n+                                    \"codebuild:BatchGetBuilds\",\n+                                ],\n+                                \"Resource\": f\"arn:aws:codebuild:{args[0]}:{args[1]}:project/*\",\n+                            }\n+                        ],\n+                    }\n+                )\n+            ),\n+            opts=pulumi.ResourceOptions(parent=self),\n+        )\n+\n+        # Create CloudWatch log group with retention to control costs\n+        log_group = make_log_group(\n+            f\"{self.name}-fn-publish-logs\",\n+            retention_days=14,\n+            parent=self,\n+        )\n+\n+        # Single CodeBuild project does both build and update\n+        publish_project = Project(\n+            f\"{self.name}-fn-publish-{pulumi.get_stack()}\",\n+            service_role=codebuild_role.arn,\n+            source=ProjectSourceArgs(\n+                type=\"CODEPIPELINE\",\n+                buildspec=json.dumps(self._buildspec()),\n+            ),\n+            artifacts=ProjectArtifactsArgs(type=\"CODEPIPELINE\"),\n+            environment=ProjectEnvironmentArgs(\n+                type=\"ARM_CONTAINER\",\n+                compute_type=\"BUILD_GENERAL1_SMALL\",\n+                image=\"aws/codebuild/amazonlinux-aarch64-standard:3.0\",\n+                environment_variables=[\n+                    ProjectEnvironmentEnvironmentVariableArgs(\n+                        name=\"FUNCTION_NAME\",\n+                        value=f\"{self.name}-{pulumi.get_stack()}\",\n+                    ),\n+                    ProjectEnvironmentEnvironmentVariableArgs(\n+                        name=\"PACKAGE_NAME\", value=self.name\n+                    ),\n+                    ProjectEnvironmentEnvironmentVariableArgs(\n+                        name=\"BUCKET_NAME\", value=build_bucket.bucket\n+                    ),\n+                    ProjectEnvironmentEnvironmentVariableArgs(\n+                        name=\"DEBUG_MODE\", value=str(self.debug_mode)\n+                    ),\n+                ],\n+            ),\n+            logs_config=ProjectLogsConfigArgs(\n+                cloudwatch_logs=ProjectLogsConfigCloudwatchLogsArgs(\n+                    status=\"ENABLED\",\n+                ),\n+            ),\n+            opts=pulumi.ResourceOptions(parent=self, depends_on=[log_group]),\n+        )","path":"infra/components/ecs_lambda.py","commit_id":"b4b5b64f14380322d789450da96a71cea99e1006","original_commit_id":"b4b5b64f14380322d789450da96a71cea99e1006","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n<details>\n<summary>üß© Analysis chain</summary>\n\nüåê Web query:\n\n`Pulumi AWS Python codebuild ProjectLogsConfigCloudwatchLogsArgs documentation group_name`\n\nüí° Result:\n\nThe ProjectLogsConfigCloudwatchLogsArgs `group_name` (a.k.a. GroupName) is the CloudWatch Logs log group name for CodeBuild logs ‚Äî a string. In Pulumi's AWS CodeBuild Project docs it‚Äôs documented as \"GroupName / groupName: Group name of the logs in CloudWatch Logs.\" [1]. This corresponds to the CodeBuild CloudWatchLogs GroupName property in AWS docs. [2]\n\nExample (Python, minimal):\nlogs_config = aws.codebuild.ProjectLogsConfigCloudwatchLogsArgs(\n    group_name=\"my-log-group\",\n    stream_name=\"my-log-stream\",\n    status=\"ENABLED\",\n)\n\nReferences:\n- Pulumi AWS CodeBuild Project (ProjectLogsConfigCloudwatchLogsArgs). [1]\n- AWS CloudFormation / CodeBuild CloudWatchLogsConfig (GroupName). [2]\n\n---\n\n\n\n</details>\n\n**Wire the created log group into the CodeBuild project by setting `group_name`**\n\nThe code creates a log group via `make_log_group(f\"{self.name}-fn-publish-logs\", ...)` but does not bind it to the CodeBuild project. The `ProjectLogsConfigCloudwatchLogsArgs` accepts a `group_name` parameter that should reference this group. Without it, CodeBuild will write to the default `/aws/codebuild/<project>` group instead, and your created log group with explicit 14-day retention will remain unused.\n\nAdd `group_name=log_group.name` to `ProjectLogsConfigCloudwatchLogsArgs`:\n\n```python\nlogs_config=ProjectLogsConfigArgs(\n    cloudwatch_logs=ProjectLogsConfigCloudwatchLogsArgs(\n        group_name=log_group.name,\n        status=\"ENABLED\",\n    ),\n),\n```\n\nAlternatively, if the retention policy is not needed, remove the `make_log_group` call entirely.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn infra/components/ecs_lambda.py around lines 699 to 741, the CloudWatch log\ngroup created by make_log_group is not attached to the CodeBuild project so\nCodeBuild will use the default log group; update the\nProjectLogsConfigCloudwatchLogsArgs to include group_name=log_group.name so the\nproject writes to the created log group (or if you intend to use the default\ngroup, remove the make_log_group call and its retention settings instead).\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:olive -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-12-06T03:48:06Z","updated_at":"2025-12-06T03:48:06Z","html_url":"https://github.com/tnorlund/Portfolio/pull/491#discussion_r2594486449","pull_request_url":"https://api.github.com/repos/tnorlund/Portfolio/pulls/491","_links":{"self":{"href":"https://api.github.com/repos/tnorlund/Portfolio/pulls/comments/2594486449"},"html":{"href":"https://github.com/tnorlund/Portfolio/pull/491#discussion_r2594486449"},"pull_request":{"href":"https://api.github.com/repos/tnorlund/Portfolio/pulls/491"}},"reactions":{"url":"https://api.github.com/repos/tnorlund/Portfolio/pulls/comments/2594486449/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":699,"original_start_line":699,"start_side":"RIGHT","line":741,"original_line":741,"side":"RIGHT","author_association":"NONE","original_position":741,"position":741,"subject_type":"line"}]