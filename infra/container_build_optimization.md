# Container Build Optimization Strategy

## Current Build Performance Issues

Building containerized Lambda functions in this project currently takes 10-20 minutes due to:
- Sequential builds (no parallelization)
- Large build contexts uploaded each time
- No cross-machine caching
- Rebuilding all dependencies on every change

## Optimization Strategies

### 1. Use Base Images for Shared Dependencies (âœ… Partially Implemented)

**Current Implementation:**
- `base_images.py` creates ECR repositories with pre-installed `receipt_dynamo` and `receipt_label`
- Reduces build time by ~60% when base image is available
- Issue: Base images can become stale (as seen with pinecone dependency)

**Improvement Needed:**
```dockerfile
# Better base image strategy
FROM base-image:${VERSION}
# Only install if packages changed
ARG PACKAGES_HASH
RUN if [ "${PACKAGES_HASH}" != "${CACHED_HASH}" ]; then \
    pip install --force-reinstall /tmp/packages; \
    fi
```

### 2. Minimize Docker Build Context

**Current Issue:** Entire repository is sent as build context

**Solution:** Created `.dockerignore` file:
```
**/__pycache__
**/*.pyc
**/.pytest_cache
**/.git
.venv/
*.md
tests/
docs/
.github/
scripts/
*.log
.DS_Store
.coverage
htmlcov/
.mypy_cache/
.ruff_cache/
*.egg-info/
dist/
build/
test_data/
receipt_data/
cdk.out/
.pulumi/
.vscode/
.idea/
```

**Impact:** Reduces context size from ~500MB to ~50MB

### 3. Optimize Dockerfile Layer Ordering

**Principle:** Put least-changing items first

```dockerfile
# Good layer ordering
FROM public.ecr.aws/lambda/python:3.12

# System dependencies (rarely change)
RUN dnf install -y gcc-c++ python3-devel && dnf clean all

# Python dependencies (occasional changes)
COPY requirements.txt .
RUN pip install -r requirements.txt

# Local packages (moderate changes)
COPY receipt_dynamo/ /tmp/receipt_dynamo/
COPY receipt_label/ /tmp/receipt_label/
RUN pip install /tmp/receipt_dynamo /tmp/receipt_label

# Handler code (frequent changes)
COPY handler.py ${LAMBDA_TASK_ROOT}/
```

### 4. Leverage Docker BuildKit Features

**Enable BuildKit for better caching:**
```bash
export DOCKER_BUILDKIT=1
```

**Use cache mounts in Dockerfile:**
```dockerfile
# syntax=docker/dockerfile:1
RUN --mount=type=cache,target=/root/.cache/pip \
    pip install chromadb numpy boto3
```

### 5. Implement Parallel Builds (ðŸš§ Not Yet Implemented)

**Current:** Pulumi builds images sequentially
**Proposed:** Build all images in parallel

```python
# Proposed implementation in chromadb_lambdas.py
import asyncio
from concurrent.futures import ThreadPoolExecutor

async def build_all_images_parallel(self):
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = []
        
        # Build all images in parallel
        for image_config in self.image_configs:
            future = executor.submit(
                self._build_single_image,
                image_config
            )
            futures.append(future)
        
        # Wait for all builds
        results = [f.result() for f in futures]
    
    return results
```

### 6. Use AWS CodeBuild for Remote Builds (ðŸš§ Proposed)

**Benefits:**
- Instant `pulumi up` (returns immediately)
- Builds happen on AWS infrastructure
- Better caching with CodeBuild cache
- No local Docker daemon required

**Implementation Example (`fast_container_image.py` created):**
```python
class FastContainerImage(ComponentResource):
    """Async container builds using CodeBuild."""
    
    def __init__(self, name: str, dockerfile_path: str, ...):
        # 1. Upload source to S3
        # 2. Trigger CodeBuild project
        # 3. Return immediately
        # 4. CodeBuild pushes to ECR
```

### 7. Smart Change Detection (âœ… Implemented in word_label_step_functions)

**Current Implementation:**
```python
def calculate_docker_context_hash(context_path: Path, dockerfile_path: Path) -> str:
    """Calculate hash of Docker context for change detection."""
    hasher = hashlib.sha256()
    
    # Hash Dockerfile
    hasher.update(dockerfile_path.read_bytes())
    
    # Hash Python files
    for py_file in context_path.rglob("*.py"):
        if not any(skip in str(py_file) for skip in [".git", "__pycache__", ".pytest_cache"]):
            hasher.update(py_file.read_bytes())
    
    return hasher.hexdigest()

# Skip build if hash unchanged
if stored_hash == current_hash and image_exists_in_ecr():
    skip_build()
```

## Performance Comparison

| Strategy | Build Time | Complexity | Status |
|----------|-----------|------------|--------|
| Baseline (no optimization) | 15-20 min | Low | Current |
| With .dockerignore | 12-15 min | Low | âœ… Implemented |
| With base images | 5-8 min | Medium | âœ… Partial |
| With parallel builds | 3-5 min | Medium | ðŸš§ Proposed |
| With CodeBuild | <1 min | High | ðŸš§ Proposed |
| With smart caching | 0 min (skip) | Medium | âœ… Implemented |

## Recommended Implementation Plan

### Phase 1: Quick Wins (1-2 days)
1. âœ… Add comprehensive `.dockerignore`
2. âœ… Implement smart change detection
3. â³ Fix Dockerfile layer ordering in all containers
4. â³ Enable Docker BuildKit by default

### Phase 2: Parallel Builds (3-5 days)
1. Refactor `ChromaDBLambdas` to support parallel builds
2. Use Python asyncio/threading for concurrent Docker builds
3. Add progress tracking for parallel builds

### Phase 3: Remote Builds (1-2 weeks)
1. Implement `FastContainerImage` component using CodeBuild
2. Create CodeBuild projects for each Lambda
3. Set up S3 source buckets and artifact caching
4. Integrate with Pulumi deployment

### Phase 4: Advanced Caching (1 week)
1. Implement content-addressable storage for layers
2. Share base layers across all Lambdas
3. Use ECR lifecycle policies for cache management
4. Add build performance metrics

## Cost-Benefit Analysis

### Current Costs
- Developer time: ~15 min/build Ã— 10 builds/day Ã— $100/hour = $250/day
- CI/CD time: ~20 min/build Ã— 20 builds/day = 400 minutes/day

### With Full Optimization
- Developer time: ~1 min/build Ã— 10 builds/day Ã— $100/hour = $17/day
- CI/CD time: ~5 min/build Ã— 20 builds/day = 100 minutes/day
- CodeBuild costs: ~$0.005/minute Ã— 100 minutes = $0.50/day

**Savings: ~$230/day or $5,750/month**

## Implementation Examples

### Example 1: Optimized Dockerfile
```dockerfile
# syntax=docker/dockerfile:1
ARG PYTHON_VERSION=3.12
FROM public.ecr.aws/lambda/python:${PYTHON_VERSION} AS base

# Cache mount for pip
RUN --mount=type=cache,target=/root/.cache/pip \
    pip install --upgrade pip

# Dependencies layer (cached unless requirements change)
FROM base AS dependencies
COPY requirements.txt .
RUN --mount=type=cache,target=/root/.cache/pip \
    pip install -r requirements.txt

# Application layer
FROM dependencies AS app
COPY receipt_dynamo/ /app/receipt_dynamo/
COPY receipt_label/ /app/receipt_label/
RUN --mount=type=cache,target=/root/.cache/pip \
    cd /app && \
    pip install ./receipt_dynamo ./receipt_label

# Final layer
FROM app
COPY handler.py ${LAMBDA_TASK_ROOT}/
CMD ["handler.main"]
```

### Example 2: Parallel Build Script
```python
#!/usr/bin/env python3
import asyncio
import subprocess
from pathlib import Path

async def build_image(name: str, dockerfile: Path, tag: str):
    """Build a single Docker image asynchronously."""
    cmd = [
        "docker", "build",
        "--platform", "linux/arm64",
        "-f", str(dockerfile),
        "-t", tag,
        "."
    ]
    
    proc = await asyncio.create_subprocess_exec(
        *cmd,
        stdout=asyncio.subprocess.PIPE,
        stderr=asyncio.subprocess.PIPE
    )
    
    stdout, stderr = await proc.communicate()
    
    if proc.returncode != 0:
        print(f"Failed to build {name}: {stderr.decode()}")
        raise Exception(f"Build failed for {name}")
    
    print(f"âœ“ Built {name}")
    return tag

async def build_all_lambdas():
    """Build all Lambda images in parallel."""
    tasks = [
        build_image("polling", Path("polling/Dockerfile"), "polling:latest"),
        build_image("compaction", Path("compaction/Dockerfile"), "compaction:latest"),
        build_image("submit", Path("submit/Dockerfile"), "submit:latest"),
        # Add more as needed
    ]
    
    results = await asyncio.gather(*tasks)
    print(f"Built {len(results)} images successfully")

if __name__ == "__main__":
    asyncio.run(build_all_lambdas())
```

## Debugging Build Performance

### Analyze Build Times
```bash
# Time each build step
DOCKER_BUILDKIT=1 docker build --progress=plain -t test .

# Show build cache usage
docker system df

# Clear build cache if needed
docker builder prune
```

### Monitor Build Context Size
```bash
# Check context size
du -sh .

# Test with dockerignore
docker build --no-cache -t test . 2>&1 | grep "Sending build context"
```

## Future Enhancements

### 1. Distributed Build Cache
- Use registry-based cache with `--cache-from` and `--cache-to`
- Share cache across team members
- Implement cache warming in CI

### 2. Multi-Stage Build Optimization
- Separate build and runtime stages
- Use `--target` to build only what's needed
- Implement layer reuse across images

### 3. Build-Time Metrics
- Track build duration per layer
- Monitor cache hit rates
- Alert on performance regressions

## Conclusion

Implementing these optimization strategies can reduce container build times from 15-20 minutes to under 1 minute, resulting in significant productivity gains and cost savings. The recommended approach is to implement optimizations incrementally, starting with quick wins and progressively adding more sophisticated solutions.

## References

- [Docker BuildKit Documentation](https://docs.docker.com/build/buildkit/)
- [AWS CodeBuild Docker Layer Caching](https://docs.aws.amazon.com/codebuild/latest/userguide/build-caching.html)
- [Pulumi Docker Provider](https://www.pulumi.com/registry/packages/docker/)
- [Best Practices for Writing Dockerfiles](https://docs.docker.com/develop/develop-images/dockerfile_best-practices/)