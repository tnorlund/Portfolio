# Pytest Optimization Guide

This guide documents the comprehensive pytest optimizations implemented to dramatically speed up test execution in CI/CD workflows, including advanced programmatic test splitting and intelligent load balancing.

## Overview of Optimizations

### 1. **Programmatic Test Splitting**
- **Tool**: Custom analysis scripts with intelligent grouping
- **Impact**: 4x speedup for integration tests (62.8min â†’ 15.8min)
- **Implementation**: 39 integration test files split into 4 balanced groups
- **Load balancing**: Each group has ~395 tests, ~16min execution time

### 2. **Parallel Test Execution**
- **Tool**: pytest-xdist with adaptive worker allocation
- **Impact**: 2-4x speedup on multi-core systems
- **Implementation**: Dynamic worker count based on test type and system resources

### 3. **Smart Test Selection**
- **Default**: Skip `end_to_end` and `slow` tests
- **Markers**: Categorize tests as `unit`, `integration`, `end_to_end`, `slow`
- **Impact**: 50-70% reduction in test suite execution time

### 4. **Test Result Caching**
- **GitHub Actions**: Cache `.pytest_cache` between runs
- **Local**: Reuse test discovery and results
- **Impact**: 10-20% speedup on subsequent runs

### 5. **Fail Fast Strategy**
- **Options**: `--maxfail=5 -x`
- **Impact**: Stop on first failure or after 5 failures
- **Benefit**: Faster feedback on broken builds

### 6. **Timeout Protection**
- **Tool**: pytest-timeout with 60s default
- **Impact**: Prevent hanging tests from blocking CI
- **Configuration**: Thread-based timeout method

### 7. **Coverage Optimization**
- **Strategy**: Disable coverage by default, enable only when needed
- **Impact**: 20-30% speedup
- **Command**: Add `--cov` flag when coverage is required

## Usage Examples

### Local Development

```bash
# Quick test runner (recommended)
./scripts/test_runner.sh receipt_dynamo
./scripts/test_runner.sh -t integration -c receipt_dynamo
./scripts/test_runner.sh -v -x receipt_label

# Advanced optimized test runner
python scripts/run_tests_optimized.py receipt_dynamo tests/unit --test-type unit
python scripts/run_tests_optimized.py receipt_dynamo tests/integration --test-type integration --coverage

# Traditional pytest (still works)
pytest
pytest --cov=receipt_dynamo --cov-report=html
pytest -m unit
pytest tests/unit/test_specific.py
pytest -v --tb=long
```

### CI/CD Workflows

The main workflow now includes:
- Parallel execution across packages
- Pytest result caching
- Optimized test discovery
- Separate coverage reporting

### Analysis and Optimization Tools

```bash
# Analyze test structure and performance
python scripts/analyze_tests.py

# Generate dynamic test matrix for CI
python scripts/generate_test_matrix.py

# Identify slow tests and optimization opportunities
python scripts/optimize_slow_tests.py

# Benchmark different configurations
python scripts/benchmark_tests.py
```

## Performance Benchmarks

Based on testing with receipt_dynamo (39 integration test files, 1,579 tests):

### Integration Test Performance
| Configuration | Time | Speedup |
|--------------|------|---------|
| Sequential (all tests) | 62.8min | 1.0x |
| 4 parallel groups | 15.8min | 4.0x |
| Group 1 (395 tests) | ~16min | - |
| Group 2 (396 tests) | ~16min | - |
| Group 3 (396 tests) | ~16min | - |
| Group 4 (392 tests) | ~15min | - |

### Unit Test Performance
| Configuration | Time | Speedup |
|--------------|------|---------|
| Baseline (sequential) | ~60s | 1.0x |
| Parallel (2 workers) | ~35s | 1.7x |
| Parallel (auto) | ~20s | 3.0x |
| Parallel + fast only | ~12s | 5.0x |
| Unit tests only | ~8s | 7.5x |

## Best Practices

1. **Mark Slow Tests**: Use `@pytest.mark.slow` for tests taking >1s
2. **Categorize Tests**: Always mark tests as unit/integration/e2e
3. **Mock External Services**: Use moto for AWS, responses for HTTP
4. **Avoid I/O in Unit Tests**: Keep unit tests pure and fast
5. **Use Fixtures Efficiently**: Scope fixtures appropriately

## Troubleshooting

### Tests Not Running in Parallel
- Ensure `pytest-xdist` is installed
- Check for `--dist=no` in configuration
- Some tests may not be parallelizable due to shared resources

### Flaky Tests with Parallel Execution
- Mark with `@pytest.mark.flaky`
- Consider using `--reruns` for retry logic
- Ensure proper test isolation

### Coverage Missing
- Coverage is disabled by default
- Add `--cov=package_name` to enable
- Check `.coveragerc` for exclusions

## Advanced Features

### Programmatic Test Splitting
- **Automatic analysis**: Scans all test files and measures complexity
- **Optimal grouping**: Uses greedy algorithm to balance test load
- **Dynamic matrix**: Adapts automatically as tests are added/removed
- **Load balancing**: Ensures even distribution across parallel jobs

### Performance Analysis
- **Slow test identification**: Finds tests taking >2s
- **Bottleneck analysis**: Identifies I/O, sleep(), loops, HTTP calls
- **Optimization suggestions**: Provides specific improvement recommendations
- **Pattern detection**: Finds common performance anti-patterns

### Test Groups (Current Optimal Distribution)

**Group 1 (395 tests, ~16min):**
- `test__receipt_word_label.py` (135 tests) - Heaviest file
- `test__receipt.py` (86 tests)
- `test__receipt_validation_category.py` (77 tests)
- Plus 7 smaller files

**Group 2 (396 tests, ~16min):**
- `test__receipt_field.py` (132 tests)
- `test__receipt_chatgpt_validation.py` (103 tests)
- `test__receipt_validation_summary.py` (47 tests)
- Plus 7 smaller files

**Group 3 (396 tests, ~16min):**
- `test__receipt_letter.py` (127 tests)
- `test__receipt_line_item_analysis.py` (104 tests)
- `test__queue.py` (47 tests)
- Plus 7 smaller files

**Group 4 (392 tests, ~15min):**
- `test__receipt_validation_result.py` (120 tests)
- `test__receipt_label_analysis.py` (109 tests)
- `test__job.py` (51 tests)
- Plus 6 smaller files

## Future Optimizations

1. **Test Impact Analysis**: Only run tests affected by code changes
2. **Micro-optimization**: Profile individual slow tests within files
3. **Smart Test Ordering**: Run previously failed tests first
4. **Incremental Testing**: Cache test results based on code signatures
5. **Distributed Testing**: Use multiple GitHub runners for even more parallelization

## Configuration Files

### Core Configuration
- `/pytest.ini` - Root configuration
- `/pytest-fast.ini` - Optimized configuration for CI
- `/conftest.py` - Global pytest optimizations

### GitHub Actions
- `/.github/workflows/main.yml` - Updated with programmatic test splitting
- `/.github/test_matrix.json` - Generated test matrix configuration

### Analysis and Optimization Scripts
- `/scripts/analyze_tests.py` - Test structure analysis
- `/scripts/run_tests_optimized.py` - Advanced test runner
- `/scripts/generate_test_matrix.py` - Dynamic matrix generation
- `/scripts/optimize_slow_tests.py` - Performance optimization suggestions
- `/scripts/test_runner.sh` - Developer-friendly local runner
- `/scripts/test_matrix.json` - Current optimal test groupings

## Monitoring Performance

Track test execution times in CI:
1. Check "Run tests" step duration in GitHub Actions
2. Review pytest `--durations` output
3. Monitor trends over time
4. Set up alerts for regression

## Rolling Back

If optimizations cause issues:
1. Remove `-n auto` to disable parallel execution
2. Remove `--maxfail` and `-x` for complete test runs
3. Re-enable coverage with `--cov`
4. Use original pytest.ini configuration